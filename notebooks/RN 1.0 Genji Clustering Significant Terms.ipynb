{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Japanese Text Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "from sudachipy import tokenizer\n",
    "from sudachipy import dictionary\n",
    "from sudachipy import config\n",
    "\n",
    "from io import StringIO\n",
    "import re\n",
    "from scipy import stats\n",
    "\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of using sudachi for parsing Japanese text.\n",
    "\n",
    "Check [github](https://github.com/WorksApplications/Sudachi) page for Japanese morphological analyzer Sudachi. ![Sudachi](images/Sudachi.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config.SETTINGFILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    settings = json.load(f)\n",
    "tokenizer_obj = dictionary.Dictionary(settings).create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.SETTINGFILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat {config.SETTINGFILE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-granular tokenization\n",
    "(following results are w/ `system_full.dic`\n",
    "you may not be able to replicate this particular example w/ `system_core.dic`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = tokenizer.Tokenizer.SplitMode.C\n",
    "[m.surface() for m in tokenizer_obj.tokenize(mode, \"医薬品安全管理責任者\")]\n",
    "# => ['医薬品', '安全', '管理責任者']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = tokenizer.Tokenizer.SplitMode.B\n",
    "[m.surface() for m in tokenizer_obj.tokenize(mode, \"医薬品安全管理責任者\")]\n",
    "# => ['医薬品', '安全', '管理', '責任者']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = tokenizer.Tokenizer.SplitMode.A\n",
    "[m.surface() for m in tokenizer_obj.tokenize(mode, \"医薬品安全管理責任者\")]\n",
    "# => ['医薬', '品', '安全', '管理', '責任', '者']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = '複数粒度の分割結果に基づく日本語単語分散表現'\n",
    "mode = tokenizer.Tokenizer.SplitMode.C\n",
    "for t in zip([m.surface() for m in tokenizer_obj.tokenize(mode, s)], [m.reading_form() for m in tokenizer_obj.tokenize(mode, s)]):\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = '分散表現の構築手法'\n",
    "mode = tokenizer.Tokenizer.SplitMode.C\n",
    "for t in zip([m.surface() for m in tokenizer_obj.tokenize(mode, s)], [m.reading_form() for m in tokenizer_obj.tokenize(mode, s)]):\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Morpheme information\n",
    "\n",
    "m = tokenizer_obj.tokenize(mode, \"食べ\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.surface() # => '食べ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.dictionary_form() # => '食べる'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.reading_form() # => 'タベ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reading_form(word):\n",
    "    m = tokenizer_obj.tokenize(mode, word)[0]\n",
    "    return m.reading_form()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.part_of_speech() # => ['動詞', '一般', '*', '*', '下一段-バ行', '連用形-一般']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_obj.tokenize(mode, \"附属\")[0].normalized_form()\n",
    "# => '付属'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_obj.tokenize(mode, \"SUMMER\")[0].normalized_form()\n",
    "# => 'サマー'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_obj.tokenize(mode, \"シュミレーション\")[0].normalized_form()\n",
    "# => 'シミュレーション'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Japanese Text Mining\n",
    "\n",
    "![Japanese Text Mining](images/japanese_text_mining.jpg)\n",
    "Check out the [Emory University workshop blog](https://scholarblogs.emory.edu/japanese-text-mining/) on Japanese Text Mining. The example notebook cells below repeat the steps in the [tutorial](http://history.emory.edu/RAVINA/JF_text_mining/Guides/Jtextmining_intro_part1.html) of Mark Ravina using python instead of R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get('http://history.emory.edu/RAVINA/JF_text_mining/Guides/data/meiroku_zasshi.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.encoding = 'utf-8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [t.split('\" \"') for t in response.text.split('\\n')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[d.replace('\"', '') for d in row] for row in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = data[1][0].split()\n",
    "d.extend(data[1][1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for d in data[1:]:\n",
    "    row = d[0].split()\n",
    "    row.extend(d[1:])\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(rows, columns = ['index', 'year', 'issue', 'title', 'author', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(df.index[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = list(df.author)\n",
    "no_boxes_per_line = 5\n",
    "[authors[no_boxes_per_line*m: no_boxes_per_line*m+no_boxes_per_line] for m in range(31)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = df.author == '西周'\n",
    "df[mask][['index', 'title']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tale of Genji: Significant Terms and Word Clusters.\n",
    "\n",
    "![源氏物語歌合](images/200014735/image/200014735_00014.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Tale of the Genji consists of 54 chapters. Each chapter is broken into sections and each section into a list of text blocks.\n",
    "with open('../data/raw/genji_data.json') as fp:\n",
    "    genji_data = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chapter_names = list(genji_data.keys())\n",
    "print(len(chapter_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section keys for a chapter.\n",
    "chapter = chapter_names[0]\n",
    "print(chapter, genji_data[chapter].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia_genji = pd.read_html('https://en.wikipedia.org/wiki/The_Tale_of_Genji', attrs={\"class\": \"wikitable\"})[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia_genji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chapter_names[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = ''\n",
    "mode = tokenizer.Tokenizer.SplitMode.C\n",
    "text_length = 0\n",
    "chapter_boundaries = []\n",
    "for chapter in chapter_names:\n",
    "    chapter_boundaries.append(text_length)\n",
    "    for section in genji_data[chapter].keys():\n",
    "        wordlist = []\n",
    "        for block in genji_data[chapter][section]:\n",
    "            wordlist = [m.dictionary_form() for m in tokenizer_obj.tokenize(mode, block)]\n",
    "            text_length += len(wordlist)\n",
    "            all_text += ' '.join(wordlist)\n",
    "            print('\\r{}'.format(text_length), end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = re.findall('\\w+', all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/processed/all_text.txt', 'w') as fp:\n",
    "    fp.write(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sents = !ruby pragmatic_segmenter_test.rb all_text.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import WordLevelStatistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class WordLevelStatistics():\n",
    "#     # Copyright 2014 Shubhanshu Mishra. All rights reserved.\n",
    "#     #\n",
    "#     # This library is free software; you can redistribute it and/or\n",
    "#     # modify it under the same terms as Python itself.\n",
    "#     def __init__(self, word_pos=None, corpus_file=None, percentile_C=95):\n",
    "#         '''This package is a port of the perl module Algorithm::WordLevelStatistics by\n",
    "#         Francesco Nidito which can be found at:\n",
    "#         http://search.cpan.org/~nids/Algorithm-WordLevelStatistics-0.03/\n",
    "\n",
    "#         The code is an implementation of the spatial statistics described in\n",
    "#         the following paper:\n",
    "#         @article{carpena2009level,\n",
    "#           title={Level statistics of words: Finding keywords in literary texts and symbolic sequences},\n",
    "#           author={Carpena, P and Bernaola-Galv{\\'a}n, P and Hackenberg, M and Coronado, AV and Oliver, JL},\n",
    "#           journal={Physical Review E},\n",
    "#           volume={79},\n",
    "#           number={3},\n",
    "#           pages={035102},\n",
    "#           year={2009},\n",
    "#           publisher={APS}\n",
    "#         }\n",
    "\n",
    "#         Author: Shubhanshu Mishra\n",
    "#         Published: December 29, 2014\n",
    "#         License: GPL3\n",
    "#         '''\n",
    "#         if percentile_C is not None:\n",
    "#             self.percentile_C = percentile_C\n",
    "\n",
    "#         if word_pos is not None:\n",
    "#             self.word_pos = word_pos\n",
    "#         elif corpus_file is not None:\n",
    "#             self.word_pos = dict()\n",
    "#             self.pos_counter = 0\n",
    "#             if isinstance(corpus_file, list):\n",
    "#                 for c in corpus_file:\n",
    "#                     self.gen_word_pos(c)\n",
    "#             else:\n",
    "#                 self.gen_word_pos(corpus_file)\n",
    "\n",
    "#     def gen_word_pos(self, corpus_file):\n",
    "#         # with open(corpus_file, encoding='utf-8') as fp:\n",
    "#         text = corpus_file.read()  # .lower()\n",
    "#         tokens = re.findall('\\w+', text)\n",
    "#         for t in tokens:\n",
    "#             if t not in self.word_pos:\n",
    "#                 self.word_pos[t] = []\n",
    "#             self.word_pos[t].append(self.pos_counter)\n",
    "#             self.pos_counter += 1\n",
    "\n",
    "#     def compute_spectra(self):\n",
    "#         if self.word_pos is None or len(self.word_pos.keys()) < 1:\n",
    "#             return None\n",
    "#         # Count total words in the text.\n",
    "#         self.tot_words = sum([len(self.word_pos[k]) for k in self.word_pos.keys()])\n",
    "\n",
    "#         # Compute level statistics of all terms\n",
    "#         self.level_stat = []\n",
    "#         for k in self.word_pos.keys():\n",
    "#             ls = self.compute_spectrum(k)\n",
    "#             self.level_stat.append(ls)\n",
    "\n",
    "#         # Sort level_stat frequency, use index in this list for vocab.\n",
    "#         self.level_stat = sorted(self.level_stat,\n",
    "#                                  key=lambda x: x['count'],\n",
    "#                                  reverse=True)\n",
    "\n",
    "#         # Add index to keep track of vocab, higher freq <-> higer index.\n",
    "#         for n, vocab_entry in enumerate(self.level_stat):\n",
    "#             vocab_entry['vocab_index'] = n\n",
    "\n",
    "#         self.threshold = stats.scoreatpercentile(    ## TODO: Compute this directly, dont import extra lib.\n",
    "#             [t['C'] for t in self.level_stat], self.percentile_C)\n",
    "#         self.level_stat_thresholded = [t for t in self.level_stat if t['C'] > self.threshold]\n",
    "\n",
    "#         # Significant terms\n",
    "#         self.significant_terms = [t['word'] for t in self.level_stat_thresholded]\n",
    "\n",
    "#     def compute_spectrum(self, word):\n",
    "#         positions = self.word_pos[word]\n",
    "#         n = len(positions)\n",
    "#         ls = {'word': word, 'count': n, 'C': 0, 'sigma_nor': 0}\n",
    "#         if n > 3:\n",
    "#             # position -> distance from preceding element in text\n",
    "#             tmp = [positions[i+1] - positions[i] for i in range(n-1)]\n",
    "#             # len(tmp) = n-1\n",
    "#             avg = sum(tmp)*1.0/(n-1)\n",
    "#             sigma = sum([(k-avg)**2 for k in tmp])*1.0/(n-1)\n",
    "#             sigma = (sigma**(0.5))/avg\n",
    "\n",
    "#             p = n*1.0/self.tot_words\n",
    "#             ls['sigma_nor'] = sigma/((1.0-p)**.5)\n",
    "\n",
    "#             ls['C'] = (ls['sigma_nor'] - (2.0*n-1.0)/(2.0*n+2.0))\\\n",
    "#                        * ((n**0.5) * (1.0+2.8*n**-0.865))\n",
    "#         return ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = StringIO(all_text)\n",
    "word_level_statistics = WordLevelStatistics(corpus_file=fp, percentile_C=98)\n",
    "word_level_statistics.compute_spectra()\n",
    "\n",
    "lvls_df = pd.DataFrame(word_level_statistics.level_stat_thresholded)\n",
    "significant_terms = word_level_statistics.significant_terms\n",
    "print('Threshold: {}, ({} percentile) find {} significant terms.'.format(\n",
    "                             word_level_statistics.threshold,\n",
    "                             word_level_statistics.percentile_C,\n",
    "                             len(significant_terms)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lvls_df = lvls_df.sort_values(by='sigma_nor', ascending=False)\n",
    "lvls_df['reading form'] = lvls_df.word.map(get_reading_form)\n",
    "lvls_df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = '斎宮'\n",
    "for n, chapter in enumerate(chapter_names):\n",
    "    for section in genji_data[chapter].keys():\n",
    "        for block in genji_data[chapter][section]:\n",
    "            if word in block:\n",
    "                print(n+1, chapter, section, block)\n",
    "                print('='*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_length(text):\n",
    "    fp = StringIO(text)\n",
    "    return len(fp.read().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keywords(text):\n",
    "    fp = StringIO(text)\n",
    "    word_level_statistics = WordLevelStatistics(corpus_file=fp, percentile_C=90)\n",
    "    word_level_statistics.compute_spectra()\n",
    "\n",
    "    lvls_df = pd.DataFrame(word_level_statistics.level_stat_thresholded)\n",
    "    try:\n",
    "        lvls_df = lvls_df.sort_values(by='sigma_nor', ascending=False)\n",
    "    except:\n",
    "        print(text)\n",
    "        return ''\n",
    "    return '|'.join(lvls_df.word[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text length'] = df['text'].map(text_length)\n",
    "df['keywords'] = df['text'].map(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = df.keywords.str.contains('スパルタ')\n",
    "df[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[55].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = df.issue == str(13)\n",
    "df[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import HTML, Image, Layout, Button, Label\n",
    "from ipywidgets import HBox, VBox, Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_terms = 30\n",
    "word_list = list(lvls_df['word'].head(no_terms))\n",
    "positions = [word_level_statistics.word_pos[word] for word in word_list]\n",
    "keywords_in_context = [' '.join(word_level_statistics.tokens[n-2:n+3]) for n,w in enumerate(word_level_statistics.tokens)]\n",
    "\n",
    "word_list.reverse()\n",
    "positions.reverse()\n",
    "\n",
    "fig1 = go.FigureWidget()\n",
    "for w, p in zip(word_list, positions):\n",
    "    scatter = fig1.add_scatter(x=p, y=[w]*len(p))\n",
    "    scatter.mode = 'markers'\n",
    "    scatter.marker.symbol = 'line-ns-open'\n",
    "    scatter.marker.color = 'grey'\n",
    "    scatter.name = w\n",
    "    scatter.hovertext = [keywords_in_context[n] for n in p]\n",
    "    scatter.hoverinfo = 'text'\n",
    "\n",
    "ticklabels = []\n",
    "for n in range(1,55):\n",
    "    if n%2 == 0:\n",
    "        ticklabels.append(str(n))\n",
    "    else:\n",
    "        ticklabels.append('')\n",
    "\n",
    "layout = go.Layout(\n",
    "    title='Word Distributions for Top {} Significant Terms'.format(no_terms),\n",
    "    showlegend=False,\n",
    "    autosize=True,\n",
    "#     width=1000,\n",
    "    height=700,\n",
    "    margin=go.layout.Margin(\n",
    "        l=50,\n",
    "        r=50,\n",
    "        b=100,\n",
    "        t=100,\n",
    "        pad=4\n",
    "    ),\n",
    "    hovermode='closest',\n",
    "#     paper_bgcolor='#7f7f7f',\n",
    "#     plot_bgcolor='#c7c7c7',\n",
    "    xaxis=dict(\n",
    "        title=None,\n",
    "        titlefont=dict(\n",
    "            family='Arial, sans-serif',\n",
    "            size=18,\n",
    "            color='lightgrey'\n",
    "        ),\n",
    "        showticklabels=True,\n",
    "#         ticks='outside',\n",
    "        tickangle=45,\n",
    "        tickfont=dict(\n",
    "            family='Old Standard TT, serif',\n",
    "            size=14,\n",
    "            color='black'\n",
    "        ),\n",
    "        tickvals=chapter_boundaries,\n",
    "        ticktext=ticklabels,\n",
    "        automargin=True,\n",
    "        showgrid=True,\n",
    "        zeroline=False,\n",
    "        showline=False,\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title=None,\n",
    "        titlefont=dict(\n",
    "            family='Arial, sans-serif',\n",
    "            size=18,\n",
    "            color='lightgrey'\n",
    "        ),\n",
    "        showticklabels=True,\n",
    "        automargin=True,\n",
    "        tickangle=0,\n",
    "        tickfont=dict(\n",
    "            family='Old Standard TT, serif',\n",
    "            size=14,\n",
    "            color='black'\n",
    "        ),\n",
    "        tickvals=word_list,\n",
    "        showgrid=True,\n",
    "        zeroline=False,\n",
    "        showline=False,\n",
    "    )\n",
    ")\n",
    "\n",
    "fig1.layout = layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lvls_df.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymagnitude\n",
    "import hdbscan\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "try:\n",
    "    import umap\n",
    "    print(\"Using: umap\")\n",
    "except ImportError:\n",
    "    import bhtsne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import enrich_significant_terms, topic_exemplars, display_topics, topic_order_index, hdbscan_parameter_search, enumerate_exemplars\n",
    "from IPython.core.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger()\n",
    "formatter = logging.Formatter('%(asctime)s %(message)s',\"%b-%d-%Y %H:%M:%S\")\n",
    "logger.handlers[0].setFormatter(formatter)\n",
    "logging.getLogger('joblib').setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# background_model = '../data/external/wiki-news-300d-1M.magnitude'\n",
    "background_model = '/Users/ray/data/models/cc.ja.300.magnitude'\n",
    "background_vectors = pymagnitude.Magnitude(background_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local_vectors = '../models/hobbit/wordvectors_rare15_spl_window5_bag_hash0_dim200_sqrt_cca_pseudo0_ce0P75_se0.magnitude'\n",
    "# local_vectors = pymagnitude.Magnitude(local_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectors = pymagnitude.Magnitude(local_vectors, background_vectors)\n",
    "# vectors = local_vectors\n",
    "vectors = background_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "significant_terms = list(lvls_df['word'])\n",
    "significant_vectors = vectors.query(significant_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    fit = umap.UMAP(n_neighbors=15, n_components=10, metric='euclidean')\n",
    "    vec_10d = fit.fit_transform(significant_vectors)\n",
    "    fit = umap.UMAP(n_neighbors=15, n_components=2, metric='euclidean')\n",
    "    vec_2d = fit.fit_transform(vec_10d)\n",
    "except Exception as ex:\n",
    "    logging.error(\"Trying bhtsne. Got exception {}\".format(ex))\n",
    "    vec_2d = bhtsne.tsne(np.asfarray(significant_vectors, dtype='float64' ),dimensions=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "significant_terms_enriched = enrich_significant_terms(lvls_df, vec_10d, vec_2d, 'leaf')\n",
    "exemplar_scores, hovers = topic_exemplars(significant_terms_enriched)\n",
    "summary = pd.DataFrame([h.split(':') for h in hovers], columns=['topic', 'terms'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(significant_terms_enriched[significant_terms_enriched['topic']==-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (lvls_df.word == '器量')\n",
    "lvls_df[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** Cache (full) results from dictionary lookups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jisho_lookup(word):\n",
    "    # word = '器量'\n",
    "    response = requests.get('https://jisho.org/api/v1/search/words?keyword={}'.format(word))\n",
    "    word_def = response.json()\n",
    "    # print(word_def)\n",
    "    jisho_definition = ''\n",
    "    try:\n",
    "        jisho_definition = ' | '.join(word_def['data'][0]['senses'][0]['english_definitions'])\n",
    "    except Exception as ex:\n",
    "        pass\n",
    "#         print(word, ex)\n",
    "    return jisho_definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lvls_df['definition'] = list(map(jisho_lookup, lvls_df['word']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_topic_map = dict(lvls_df[['word', 'topic']].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = 'やっかい'\n",
    "neighbors = [(word, 1.0)]\n",
    "neighbors.extend(background_vectors.most_similar_approx(word, topn=15))\n",
    "for w,s in neighbors:\n",
    "    t = '_'\n",
    "    if w in word_topic_map:\n",
    "        t = word_topic_map[w]\n",
    "    print(w, t, s, jisho_lookup(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbrs = NearestNeighbors(n_neighbors=15, algorithm='ball_tree').fit(vec_10d)\n",
    "distances, indices = nbrs.kneighbors(vec_10d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_no = 5\n",
    "for n, word_num in enumerate(indices[index_no]):\n",
    "    if distances[index_no][n] < 0.6:\n",
    "        w = significant_terms[word_num]\n",
    "        t = '_'\n",
    "        if w in word_topic_map:\n",
    "            t = word_topic_map[w]\n",
    "        print(w, t, distances[index_no][n], jisho_lookup(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_no = 7\n",
    "mask = (lvls_df.topic == topic_no)\n",
    "lvls_df[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics, top_columns = display_topics(significant_terms_enriched, n_rows=20, n_cols=35)\n",
    "topics = topics.fillna('')\n",
    "print('{} topics'.format(significant_terms_enriched['topic'].max()))\n",
    "display(HTML(topics.to_html(index=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = list(lvls_df[mask]['word'])\n",
    "positions = [word_level_statistics.word_pos[word] for word in word_list]\n",
    "keywords_in_context = [' '.join(word_level_statistics.tokens[n-2:n+3]) for n,w in enumerate(word_level_statistics.tokens)]\n",
    "\n",
    "word_list.reverse()\n",
    "positions.reverse()\n",
    "\n",
    "fig = go.FigureWidget()\n",
    "for w, p in zip(word_list, positions):\n",
    "    scatter = fig.add_scatter(x=p, y=[w]*len(p))\n",
    "    scatter.mode = 'markers'\n",
    "    scatter.marker.symbol = 'line-ns-open'\n",
    "    scatter.marker.color = 'grey'\n",
    "    scatter.name = w\n",
    "    scatter.hovertext = [keywords_in_context[n] for n in p]\n",
    "    scatter.hoverinfo = 'text'\n",
    "\n",
    "ticklabels = []\n",
    "for n in range(1,55):\n",
    "    if n%2 == 0:\n",
    "        ticklabels.append(str(n))\n",
    "    else:\n",
    "        ticklabels.append('')\n",
    "\n",
    "layout = go.Layout(\n",
    "    title='Word Distributions for Topic {}'.format(topic_no),\n",
    "    showlegend=False,\n",
    "    autosize=True,\n",
    "#     width=1000,\n",
    "    height=700,\n",
    "    margin=go.layout.Margin(\n",
    "        l=50,\n",
    "        r=50,\n",
    "        b=100,\n",
    "        t=100,\n",
    "        pad=4\n",
    "    ),\n",
    "#     paper_bgcolor='#7f7f7f',\n",
    "#     plot_bgcolor='#c7c7c7',\n",
    "    xaxis=dict(\n",
    "        title=None,\n",
    "        titlefont=dict(\n",
    "            family='Arial, sans-serif',\n",
    "            size=18,\n",
    "            color='lightgrey'\n",
    "        ),\n",
    "        showticklabels=True,\n",
    "#         ticks='outside',\n",
    "        tickangle=45,\n",
    "        tickfont=dict(\n",
    "            family='Old Standard TT, serif',\n",
    "            size=14,\n",
    "            color='black'\n",
    "        ),\n",
    "        tickvals=chapter_boundaries,\n",
    "        ticktext=ticklabels,\n",
    "        automargin=True,\n",
    "        showgrid=True,\n",
    "        zeroline=False,\n",
    "        showline=False,\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title=None,\n",
    "        titlefont=dict(\n",
    "            family='Arial, sans-serif',\n",
    "            size=18,\n",
    "            color='lightgrey'\n",
    "        ),\n",
    "        showticklabels=True,\n",
    "        automargin=True,\n",
    "        tickangle=0,\n",
    "        tickfont=dict(\n",
    "            family='Old Standard TT, serif',\n",
    "            size=14,\n",
    "            color='black'\n",
    "        ),\n",
    "        tickvals=word_list,\n",
    "        showgrid=True,\n",
    "        zeroline=False,\n",
    "        showline=False,\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.layout = layout\n",
    "fig.layout.hovermode = 'closest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lvls_df[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
