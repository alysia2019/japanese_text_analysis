{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Textual Analysis of the Hobbit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tolkien Books in my Library \n",
    "\n",
    "![texte](images/tolkien_shelf.JPG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "import sqlite3\n",
    "import os\n",
    "from math import*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(\"/Users/ray/Calibre Library/metadata.db\")\n",
    "\n",
    "books_df = pd.read_sql_query(\"select * from books;\", conn)\n",
    "authors_df = pd.read_sql_query(\"select * from authors;\", conn)\n",
    "publishers_df = pd.read_sql_query(\"select * from publishers;\", conn)\n",
    "books_authors_df = pd.read_sql_query(\"select * from books_authors_link;\", conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title                                                                     Id\n",
      "----------------------------------------------------------------------  ----\n",
      "The Hobbit: 75th Anniversary Edition                                      61\n",
      "The History of the Hobbit: Mr Baggins and Return to Bag-End              123\n",
      "The Legend of Sigurd and Gudrún                                          127\n",
      "The Book of Lost Tales, Part Two: Part Two (History of Middle-Earth 2)   270\n",
      "The Book of Lost Tales, Part One (History of Middle-Earth 1)             290\n",
      "Unfinished Tales of Numenor and Middle-Earth                             392\n",
      "The Lord of the Rings: One Volume                                        393\n",
      "The Hobbit: Illustrated by Alan Lee                                      394\n",
      "Beowulf: A Translation and Commentary, Together With Sellic Spell        395\n",
      "The Silmarillion                                                         396\n",
      "The Children of Húrin                                                    397\n"
     ]
    }
   ],
   "source": [
    "author_ids = list(authors_df[authors_df.name.str.contains('Tol')]['id'])\n",
    "book_ids = list(books_authors_df[books_authors_df.author.isin(author_ids)]['book'])\n",
    "\n",
    "print(tabulate(books_df[books_df.id.isin(book_ids)][['title','id']], \n",
    "                        showindex='never', headers=['Title','Id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hobbit_path = list(books_df[books_df['id']==61]['path'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ray/Calibre Library/J. R. R. Tolkien/The Hobbit_ 75th Anniversary Edition (61)/The Hobbit_ 75th Anniversary Edition - J. R. R. Tolkien.txt\n"
     ]
    }
   ],
   "source": [
    "for p in Path(\"/Users/ray/Calibre Library/\" + hobbit_path).glob('*.txt'):\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "text = p.read_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breaking the Text into Parts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![The Hobbit_ 75th Anniversary Edition](images/hobbit_cover.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from src.models import normalize\n",
    "import io\n",
    "\n",
    "from segtok.segmenter import split_single, split_multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from segtok.segmenter import split_single, split_multi\n",
    "# from segtok.tokenizer import symbol_tokenizer, word_tokenizer, web_tokenizer\n",
    "# from segtok.tokenizer import split_possessive_markers, split_contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56 Chapter I - An Unexpected Party\n",
      "74 Chapter II - Roast Mutton\n",
      "86 Chapter III - A Short Rest\n",
      "92 Chapter IV - Over Hill and under Hill\n",
      "98 Chapter V - Riddles in the Dark\n",
      "110 Chapter VI - Out of the Frying-Pan into the Fire\n",
      "116 Chapter VII - Queer Lodgings\n",
      "122 Chapter VIII - Flies and Spiders\n",
      "128 Chapter IX - Barrels Out of Bond\n",
      "134 Chapter X - A Warm Welcome\n",
      "140 Chapter XI - On the Doorstep\n",
      "146 Chapter XII - Inside Information\n",
      "152 Chapter XIII - Not at Home\n",
      "158 Chapter XIV - Fire and Water\n",
      "164 Chapter XV - The Gathering of the Clouds\n",
      "170 Chapter XVI - A Thief in the Night\n",
      "176 Chapter XVII - The Clouds Burst\n",
      "182 Chapter XVIII - The Return Journey\n",
      "188 Chapter XIX - The Last Stage\n",
      "612 Chapter I\n",
      "1102 Chapter II\n",
      "1420 Chapter III\n",
      "1612 Chapter IV\n",
      "1774 Chapter V\n",
      "2192 Chapter VI\n",
      "2466 Chapter VII\n",
      "2878 Chapter VIII\n",
      "3224 Chapter IX\n",
      "3452 Chapter X\n",
      "3624 Chapter XI\n",
      "3726 Chapter XII\n",
      "3954 Chapter XIII\n",
      "4112 Chapter XIV\n",
      "4212 Chapter XV\n",
      "4414 Chapter XVI\n",
      "4524 Chapter XVII\n",
      "4670 Chapter XVIII\n",
      "4790 Chapter XIX\n",
      "5096 Chapter 1\n"
     ]
    }
   ],
   "source": [
    "for n, line in enumerate(text.split('\\n')):\n",
    "    match = re.findall(r'^Chapter', line)\n",
    "    if match:\n",
    "        print(n, line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5726\n"
     ]
    }
   ],
   "source": [
    "text_lines = text.split('\\n')\n",
    "print(len(text_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "chapters = {1:{'start':612,  'end':1102, 'title':'An Unexpected Party'},\n",
    "            2:{'start':1102, 'end':1420, 'title':'Roast Mutton'},\n",
    "            3:{'start':1420, 'end':1612, 'title':'A Short Rest'},\n",
    "            4:{'start':1612, 'end':1774, 'title':'Over Hill and under Hill'},\n",
    "            5:{'start':1774, 'end':2192, 'title':'Riddles in the Dark'},\n",
    "            6:{'start':2192, 'end':2466, 'title':'Out of the Frying-Pan into the Fire'},\n",
    "            7:{'start':2466, 'end':2878, 'title':'Queer Lodgings'},\n",
    "            8:{'start':2878, 'end':3224, 'title':'Flies and Spiders'},\n",
    "            9:{'start':3224, 'end':3452, 'title':'Barrels Out of Bond'},\n",
    "           10:{'start':3452, 'end':3624, 'title':'A Warm Welcome'},\n",
    "           11:{'start':3624, 'end':3726, 'title':'On the Doorstep'},\n",
    "           12:{'start':3726, 'end':3954, 'title':'Inside Information'},\n",
    "           13:{'start':3954, 'end':4112, 'title':'Not at Home'},\n",
    "           14:{'start':4112, 'end':4212, 'title':'Fire and Water'},\n",
    "           15:{'start':4212, 'end':4414, 'title':'The Gathering of the Clouds'},\n",
    "           16:{'start':4414, 'end':4524, 'title':'A Thief in the Night'},\n",
    "           17:{'start':4524, 'end':4670, 'title':'The Clouds Burst'},\n",
    "           18:{'start':4670, 'end':4790, 'title':'The Return Journey'},\n",
    "           19:{'start':4790, 'end':5096-50, 'title':'The Last Stage'},  \n",
    "                                              # Last 50 lines of Chapter actually are back matter.\n",
    "                                              # Back matter includes 1st chapter of Lord of the Rings.\n",
    "           'front_matter':{'start':0, 'end':612, 'title':'Front Matter'},\n",
    "           'back_matter':{'start':5096-50, 'end':5726, 'title':'Back Matter'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SimpleTokenizer(text):\n",
    "    filter_terms = set(['Tolkien', 'Caption', 'J.R.R.', 'J.R.R.Tolkien', 'Image', 'audio'])\n",
    "    sentences = []\n",
    "    for sentence in split_single(text):\n",
    "        # tokens = split_contractions(word_tokenizer(sentence))\n",
    "        # sentence = ' '.join(tokens)\n",
    "        if set(sentence.split()).intersection(filter_terms):\n",
    "            continue\n",
    "        sentence_clean = normalize.tokenize(s=sentence, lang='en', tokenizer='elasticsearch')\n",
    "        sentences.append((sentence, sentence_clean)) \n",
    "        # print(sentence_clean)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chapter_text = '\\n'.join(chapter.split('\\n')[3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = chapters[1]['start']\n",
    "end = chapters[19]['end']\n",
    "all_text = text_lines[start: end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Jun-28-2019 12:01:42 GET http://localhost:9200/en/_analyze [status:404 request:0.129s]\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "NotFoundError(404, 'index_not_found_exception', 'no such index', en, index_expression)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-ab14918a574f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mparagraph\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                 \u001b[0mparagraph_clean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimpleTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparagraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mpair\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparagraph_clean\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                     \u001b[0mflat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-75bf94cb9674>\u001b[0m in \u001b[0;36mSimpleTokenizer\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter_terms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0msentence_clean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'elasticsearch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0msentences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence_clean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# print(sentence_clean)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/scratch/japanese_text_analysis/src/models/normalize.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(s, lang, drop_punctuation, tokenizer, es, host, port)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;31m# TODO: fail gracefully if elasticsearch isn't running?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"analyzer\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"text\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;31m# TODO: better error handling. Retrys, etc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/japanese_text_analysis/lib/python3.7/site-packages/elasticsearch/client/utils.py\u001b[0m in \u001b[0;36m_wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                     \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/japanese_text_analysis/lib/python3.7/site-packages/elasticsearch/client/indices.py\u001b[0m in \u001b[0;36manalyze\u001b[0;34m(self, index, body, params)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \"\"\"\n\u001b[1;32m     19\u001b[0m         return self.transport.perform_request(\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0;34m\"GET\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_make_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_analyze\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         )\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/japanese_text_analysis/lib/python3.7/site-packages/elasticsearch/transport.py\u001b[0m in \u001b[0;36mperform_request\u001b[0;34m(self, method, url, headers, params, body)\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0mdelay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mattempt\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m                 \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperform_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTransportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/japanese_text_analysis/lib/python3.7/site-packages/elasticsearch/connection/http_urllib3.py\u001b[0m in \u001b[0;36mperform_request\u001b[0;34m(self, method, url, params, body, timeout, ignore, headers)\u001b[0m\n\u001b[1;32m    237\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mduration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m             )\n\u001b[0;32m--> 239\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m         self.log_request_success(\n",
      "\u001b[0;32m~/miniconda3/envs/japanese_text_analysis/lib/python3.7/site-packages/elasticsearch/connection/base.py\u001b[0m in \u001b[0;36m_raise_error\u001b[0;34m(self, status_code, raw_data)\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Undecodable raw error response from server: %s'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTP_EXCEPTIONS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTransportError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_message\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madditional_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: NotFoundError(404, 'index_not_found_exception', 'no such index', en, index_expression)"
     ]
    }
   ],
   "source": [
    "# Put in separate data prep script, only needs to be run when regenerating data.\n",
    "\n",
    "if True:\n",
    "    with open('../data/raw/hobbit_flat.txt', 'w') as flat:\n",
    "        with open('../data/raw/hobbit_flat_clean.txt', 'w') as clean:\n",
    "            for paragraph in all_text:\n",
    "                if paragraph == '':\n",
    "                    continue\n",
    "                paragraph_clean = SimpleTokenizer(paragraph)\n",
    "                for pair in paragraph_clean:\n",
    "                    flat.write(pair[0] + '\\n')\n",
    "                    clean.write(pair[1] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!grep Tolkien ../data/raw/hobbit_flat.txt\n",
    "!grep Caption ../data/raw/hobbit_flat.txt\n",
    "!grep 'J.R.R.' ../data/raw/hobbit_flat.txt\n",
    "!grep Image ../data/raw/hobbit_flat.txt\n",
    "!grep audio ../data/raw/hobbit_flat.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Level Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import WordLevelStatistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chapt_no = 5\n",
    "# chapter = '\\n'.join(text_lines[chapters[chapt_no]['start']: chapters[chapt_no]['end']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = chapters[1]['start']\n",
    "# end = chapters[19]['end']\n",
    "# all_text = '\\n'.join(text_lines[start: end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_text = normalize.tokenize(s=all_text, lang='en', tokenizer='elasticsearch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/raw/hobbit_flat_clean.txt') as fp:\n",
    "    all_text = fp.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = all_text.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [s.replace('-', ' ') for s in sentences]\n",
    "sentences = [s.replace('-', ' ') for s in sentences]\n",
    "phrases = Phrases([s.split() for s in sentences], min_count=3, threshold=10)\n",
    "bigram = Phraser(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_bigrams = [' '.join(s) for s in bigram[[s.split() for s in sentences]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hobbit_flat_clean_bigrams.txt', 'w') as fp:\n",
    "    for s in sentences_bigrams:\n",
    "        fp.write(s + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hobbit_flat_clean_bigrams.txt') as fp:\n",
    "    all_text = fp.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fp = io.StringIO(all_text)\n",
    "word_level_statistics = WordLevelStatistics(corpus_file=fp, percentile_C=95)\n",
    "word_level_statistics.compute_spectra()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lvls_df = pd.DataFrame(word_level_statistics.level_stat_thresholded)\n",
    "significant_terms = word_level_statistics.significant_terms\n",
    "print('With threshold = {}, ({} percentile) find {} significant terms.'.format(\n",
    "    word_level_statistics.threshold, word_level_statistics.percentile_C, len(significant_terms)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lvls_df = lvls_df.sort_values(by='sigma_nor', ascending=False)\n",
    "lvls_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lvls_df = lvls_df[lvls_df['count'] < 300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib\n",
    "# import matplotlib.pyplot as plot\n",
    "import plotly_express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot.style.available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot.rcParams[\"figure.figsize\"] = [10, 10]\n",
    "# plot.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [{'term':term, 'count':len(word_level_statistics.word_pos[term])} for term in word_level_statistics.word_pos]\n",
    "vocab = pd.DataFrame(vocab)\n",
    "vocab = vocab.sort_values(by='count', ascending=False)\n",
    "vocab['index'] = list(range(0,len(vocab)))\n",
    "# vocab.plot(kind='scatter', x='index', y='count', loglog=True, xlim=(1,10000));\n",
    "px.scatter(vocab, x='index', y='count', log_x=True, log_y=True, hover_name='term')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = re.findall('\\w+', all_text)\n",
    "chapter_boundaries = [(n, tokens[n], tokens[n+1]) for n in word_level_statistics.word_pos['chapter']\n",
    "                                                  if tokens[n+1] not in ['you', 'beginning']]\n",
    "chapter_labels = [str(n) for n in range(1, 20)]\n",
    "# chapter_labels = ['']\n",
    "chapter_labels = [chapters[n]['title'] for n in range(1, 20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_markers = [b for (b,c1,c2) in chapter_boundaries]\n",
    "\n",
    "end_markers = [b for (b,c1,c2) in chapter_boundaries][1:]\n",
    "end_markers.append(word_level_statistics.tot_words)\n",
    "\n",
    "chapters = [' '.join(tokens[start:end]) for (start,end) in zip(start_markers, end_markers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot.rcParams[\"figure.figsize\"] = [15,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no_terms = 30\n",
    "# word_list = list(lvls_df['word'].head(no_terms))\n",
    "# positions = [word_level_statistics.word_pos[word] for word in word_list]\n",
    "# fig, ax = plot.subplots()\n",
    "# ax.eventplot(positions, linelengths=[0.5]*len(word_list))\n",
    "# plot.title('Word Distributions for Top {} Significant Terms'.format(no_terms));\n",
    "# plot.xticks([c[0] for c in chapter_boundaries], chapter_labels, rotation=45, ha='right')\n",
    "# plot.yticks(range(0, len(word_list)), word_list);\n",
    "# plot.gca().invert_yaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Plotly for word usage trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_terms = 30\n",
    "word_list = list(lvls_df['word'].head(no_terms))\n",
    "positions = [word_level_statistics.word_pos[word] for word in word_list]\n",
    "keywords_in_context = [' '.join(word_level_statistics.tokens[n-2:n+3]) for n,w in enumerate(word_level_statistics.tokens)]\n",
    "\n",
    "word_list.reverse()\n",
    "positions.reverse()\n",
    "\n",
    "fig1 = go.FigureWidget()\n",
    "fig1.layout.hovermode = 'closest'\n",
    "for w, p in zip(word_list, positions):\n",
    "    scatter = fig1.add_scatter(x=p, y=[w]*len(p))\n",
    "    scatter.mode = 'markers'\n",
    "    scatter.marker.symbol = 'line-ns-open'\n",
    "    scatter.marker.color = 'grey'\n",
    "    scatter.name = w\n",
    "    scatter.hovertext = [keywords_in_context[n] for n in p]\n",
    "    scatter.hoverinfo = 'text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout = go.Layout(\n",
    "    title='Word Distributions for Top {} Significant Terms'.format(no_terms),\n",
    "    showlegend=False,\n",
    "    autosize=False,\n",
    "    width=1100,\n",
    "    height=700,\n",
    "    margin=go.layout.Margin(\n",
    "        l=50,\n",
    "        r=50,\n",
    "        b=100,\n",
    "        t=100,\n",
    "        pad=4\n",
    "    ),\n",
    "#     paper_bgcolor='#7f7f7f',\n",
    "#     plot_bgcolor='#c7c7c7',\n",
    "    xaxis=dict(\n",
    "        title=None,\n",
    "        titlefont=dict(\n",
    "            family='Arial, sans-serif',\n",
    "            size=18,\n",
    "            color='lightgrey'\n",
    "        ),\n",
    "        showticklabels=True,\n",
    "        tickangle=45,\n",
    "        tickfont=dict(\n",
    "            family='Old Standard TT, serif',\n",
    "            size=14,\n",
    "            color='black'\n",
    "        ),\n",
    "        tickvals=[c[0] for c in chapter_boundaries],\n",
    "        ticktext=chapter_labels,\n",
    "        automargin=True,\n",
    "        showgrid=True,\n",
    "        zeroline=False,\n",
    "        showline=False,\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title=None,\n",
    "        titlefont=dict(\n",
    "            family='Arial, sans-serif',\n",
    "            size=18,\n",
    "            color='lightgrey'\n",
    "        ),\n",
    "        showticklabels=True,\n",
    "        automargin=True,\n",
    "        tickangle=0,\n",
    "        tickfont=dict(\n",
    "            family='Old Standard TT, serif',\n",
    "            size=14,\n",
    "            color='black'\n",
    "        ),\n",
    "        tickvals=word_list,\n",
    "        showgrid=True,\n",
    "        zeroline=False,\n",
    "        showline=False,\n",
    "    )\n",
    ")\n",
    "\n",
    "fig1.layout = layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymagnitude\n",
    "import hdbscan\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "try:\n",
    "    import umap\n",
    "    print(\"Using: umap\")\n",
    "except ImportError:\n",
    "    import bhtsne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import enrich_significant_terms, topic_exemplars, display_topics, topic_order_index, hdbscan_parameter_search, enumerate_exemplars\n",
    "from IPython.core.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger()\n",
    "formatter = logging.Formatter('%(asctime)s %(message)s',\"%b-%d-%Y %H:%M:%S\")\n",
    "logger.handlers[0].setFormatter(formatter)\n",
    "logging.getLogger('joblib').setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# background_model = '../data/external/wiki-news-300d-1M.magnitude'\n",
    "background_model = '../data/external/elmo_2x1024_128_2048cnn_1xhighway_weights.magnitude'\n",
    "background_vectors = pymagnitude.Magnitude(background_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_vectors = '../models/hobbit/wordvectors_rare15_spl_window5_bag_hash0_dim200_sqrt_cca_pseudo0_ce0P75_se0.magnitude'\n",
    "local_vectors = pymagnitude.Magnitude(local_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectors = pymagnitude.Magnitude(local_vectors, background_vectors)\n",
    "# vectors = local_vectors\n",
    "vectors = background_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "significant_terms = list(lvls_df['word'])\n",
    "significant_vectors = vectors.query(significant_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    fit = umap.UMAP(n_neighbors=15, n_components=10, metric='euclidean')\n",
    "    vec_10d = fit.fit_transform(significant_vectors)\n",
    "    fit = umap.UMAP(n_neighbors=15, n_components=2, metric='euclidean')\n",
    "    vec_2d = fit.fit_transform(vec_10d)\n",
    "except Exception as ex:\n",
    "    logging.error(\"Trying bhtsne. Got exception {}\".format(ex))\n",
    "    vec_2d = bhtsne.tsne(np.asfarray(significant_vectors, dtype='float64' ),dimensions=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "significant_terms_enriched = enrich_significant_terms(lvls_df, vec_10d, vec_2d, 'leaf')\n",
    "exemplar_scores, hovers = topic_exemplars(significant_terms_enriched)\n",
    "summary = pd.DataFrame([h.split(':') for h in hovers], columns=['topic', 'terms'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(significant_terms_enriched[significant_terms_enriched['topic']==-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics, top_columns = display_topics(significant_terms_enriched, n_rows=20, n_cols=35)\n",
    "topics = topics.fillna('')\n",
    "print('{} topics'.format(significant_terms_enriched['topic'].max()))\n",
    "display(HTML(topics.to_html(index=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence  = vectors.query([\"play\", \"some\", \"music\", \"on\", \"the\", \"living\", \"room\", \"speakers\", \".\"])\n",
    "# Returns: an array of size (9 (number of words) x 768 (3 ELMo components concatenated))\n",
    "unrolled = vectors.unroll(sentence)\n",
    "# Returns: an array of size (3 (each ELMo component) x 9 x 256 (the number of dimensions for each ELMo component))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "unrolled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hobbit_flat_clean_bigrams.txt') as fp:\n",
    "    sents = fp.readlines()\n",
    "    sents = [s.strip() for s in sents]\n",
    "sent_ids = range(len(sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(significant_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = {}\n",
    "for sent_id, sent in enumerate(sents):\n",
    "    filtered_sent = list(filter(lambda x: x in set(significant_terms), sent.split()))\n",
    "    if len(filtered_sent) == 0:\n",
    "        continue\n",
    "    sentence  = vectors.query(filtered_sent)\n",
    "    unrolled = vectors.unroll(sentence)\n",
    "    for word_pos, word in enumerate(filtered_sent):\n",
    "        key = word + '_' + str(sent_id) + '_' + str(word_pos)\n",
    "        word_vectors[key] = unrolled[word_pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "significant_vectors = [word_vectors[key] for key in word_vectors]\n",
    "significant_words = [key for key in word_vectors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    fit = umap.UMAP(n_neighbors=15, n_components=10, metric='euclidean')\n",
    "    vec_10d = fit.fit_transform(significant_vectors)\n",
    "    fit = umap.UMAP(n_neighbors=15, n_components=2, metric='euclidean')\n",
    "    vec_2d = fit.fit_transform(vec_10d)\n",
    "except Exception as ex:\n",
    "    logging.error(\"Trying bhtsne. Got exception {}\".format(ex))\n",
    "    vec_2d = bhtsne.tsne(np.asfarray(significant_vectors, dtype='float64' ),dimensions=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lvls_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_to_weight = dict(zip(lvls_df.word, significant_terms_enriched.sigma_nor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = []\n",
    "for word_ in significant_words:\n",
    "    word = '_'.join(word_.split('_')[0:-2])\n",
    "    tmp.append((word_, word, word_to_weight[word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lvls_extended_df = pd.DataFrame(tmp, columns=['word_sent_pos', 'word', 'weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentences_JMC(parameters=None, text_data=None,\n",
    "        text_no=0, text_id=None, weights=None):\n",
    "    '''\n",
    "    OCCAMS needs the names of the data files, the dimensions (`MxN`) of the\n",
    "    term-sentence matrix, and the size of the summary.\n",
    "\n",
    "    The sparse term-sentence matrix is input as a list of sentence-id, term\n",
    "    pairs (`sets.txt`, with one pair per line). Internally, the data is stored\n",
    "    in an array such that the i-th element points to an array of integers\n",
    "    holding the i-th sentence.\n",
    "\n",
    "    The `sets_lengths` array describes the costs of sets or lengths of\n",
    "    sentences. Populated by the `read_lengths` routine that parses the\n",
    "    `lengths.txt` file -- one lenght for each sentence.\n",
    "\n",
    "    Parameters:\n",
    "    * non-zeros\n",
    "    * number of terms\n",
    "    * number of sets\n",
    "    * summary length\n",
    "    * weights\n",
    "    * lengths\n",
    "    * lower bound\n",
    "    '''\n",
    "\n",
    "    occams_path = parameters['occams_path']\n",
    "    lower_bound = parameters['lower_bound']\n",
    "    summary_length = parameters['summary_length']\n",
    "    non_zeros = parameters['non_zeros']\n",
    "\n",
    "    ## Assumption that the text_id array will be enumerated then both the\n",
    "    ## position in the array and the text_id will be passed into this routine.\n",
    "    target_data = text_data.tmp_dir + '/' + text_id + '.txt'\n",
    "    logging.info('Extract sentences from %d: target_data',\n",
    "            len(text_data.sentences_nouns[text_id]))\n",
    "    lines = LineSentence(target_data)\n",
    "    dictionary = corpora.Dictionary(lines)\n",
    "    logging.info('Dictionary details for text %s: %d, %s',\n",
    "            target_data, len(dictionary), dictionary)\n",
    "\n",
    "#     filtered_sent = list(filter(lambda x: x in set(significant_terms), sent.split()))\n",
    "    sentences_nouns = [s.split() for s in text_data.sentences_nouns[text_id]]\n",
    "    lengths_data = text_data.tmp_dir + '/' + text_id + '.lengths.txt'\n",
    "    with open(lengths_data,'w', encoding='utf-8') as fp:\n",
    "        for sentence_id, sentence in enumerate(sentences_nouns):\n",
    "            fp.write(str(len(sentence)) + '\\n')\n",
    "\n",
    "    sets_data = text_data.tmp_dir + '/' + text_id + '.sets.txt'\n",
    "    with open(sets_data,'w', encoding='utf-8') as fp:\n",
    "        for sentence_id, sentence in enumerate(sentences_nouns):\n",
    "            #print len(sentence), sentence\n",
    "            term_ids = [w[0] for w in dictionary.doc2bow(sentence)]\n",
    "            for term_id in term_ids:\n",
    "                fp.write(str(sentence_id+1) + ' ' + str(term_id+1) + '\\n')\n",
    "\n",
    "    vocab_data = text_data.tmp_dir + '/' + text_id + '.vocab.txt'\n",
    "    with open(vocab_data,'w', encoding='utf-8') as fp:\n",
    "        for n in range(0,len(dictionary)):\n",
    "            w = dictionary.get(n)\n",
    "            try:\n",
    "                fp.write(str(weights[w])+'\\n')\n",
    "            except:\n",
    "                #logging.error('Bad weight for word: %s',w)\n",
    "                fp.write(str(0) + '\\n')\n",
    "\n",
    "    nz = non_zeros\n",
    "    M = len(dictionary)\n",
    "    N = len(sentences_nouns)\n",
    "    s = summary_length\n",
    "    D = sets_data\n",
    "    W = vocab_data\n",
    "    L = lengths_data\n",
    "    b = lower_bound\n",
    "    cmd_path = occams_path\n",
    "    cmd = cmd_path + ' -z {} -m {} -n {} -s {} -D {} -W {} -L {} -b {}'.format(nz, M, N, s, D, W, L, b)\n",
    "    logging.info('OCCAMS cmd %s: ', cmd)\n",
    "    cmd_list = cmd.split()\n",
    "    ret_val = subprocess.getoutput(cmd) #, stderr=subprocess.STDOUT)\n",
    "    for line in ret_val.split('\\n'):\n",
    "        if 'Chosen sentences' in line:\n",
    "            logging.info(line)\n",
    "            sentence_ids = line.split(':')[1].split()\n",
    "\n",
    "    summary_nouns = []\n",
    "    for n in [int(n)-1 for n in sentence_ids]:\n",
    "        summary_nouns.append(' '.join(text_data.sentences_nouns[text_id][n].split()))\n",
    "        #print sample_data.sentences_nouns[0][n].split()\n",
    "        #print n, '|'.join(sample_data.sentences_nouns[0][n].split())\n",
    "\n",
    "    summary_nouns = set([w for line in summary_nouns for w in line.split()])\n",
    "\n",
    "    sids = [int(sid)-1 for sid in sentence_ids]\n",
    "\n",
    "    summary_data = text_data.tmp_dir + '/' + text_id + '.summary.txt'\n",
    "    with open(summary_data,'w', encoding='utf-8') as fp:\n",
    "        for sid in sids:\n",
    "            s = text_data.text_sentences[text_id][sid]\n",
    "            fp.write(s + '\\n')\n",
    "            #logging.info(str(sid)+': ', text_data.text_sentences[text_id][sid])\n",
    "\n",
    "    return sids, summary_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(df, n_rows=10, n_cols=12):\n",
    "    \"\"\"Pretty-print table of themes and some corpus statistics.\"\"\"\n",
    "\n",
    "#     exemplar_scores, hovers = topic_exemplars(df)\n",
    "#     top_columns = sorted(range(len(exemplar_scores)),\n",
    "#                          key=lambda i: exemplar_scores[i],\n",
    "#                          reverse=True)[:n_cols]\n",
    "\n",
    "    topics = df.pivot(index='pos', columns='topic', values='word_sent_pos')\n",
    "                      #values='word*').replace([None], [''], regex=True)\n",
    "\n",
    "    topics_display = topics[range(n_cols)].head(n_rows)\n",
    "\n",
    "    return topics_display, top_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_order_index(topic_list):\n",
    "    '''\n",
    "    The input is a list of integers (topics) that has many repeats but has been\n",
    "    sorted in a meaningful way (e.g by some word importance score). Three\n",
    "    topics might look, for example, like [1, 1, 2, 1, 3, 2, 2, 3] and this\n",
    "    routine produces an index to keep track of the topic\n",
    "    order => [1, 2, 1, 3, 1, 2, 3, 2].\n",
    "    '''\n",
    "    position_counter = Counter()\n",
    "    per_topic_index = []\n",
    "    for t in topic_list:\n",
    "        position_counter[t] += 1\n",
    "        per_topic_index.append(position_counter[t])\n",
    "    return per_topic_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=10,\n",
    "                            min_samples=10,\n",
    "                            approx_min_span_tree=False,\n",
    "                            cluster_selection_method='leaf')\n",
    "labels = clusterer.fit_predict(np.array(vec_10d))\n",
    "lvls_extended_df['topic'] = labels\n",
    "\n",
    "topic_list = list(lvls_extended_df['topic'])\n",
    "lvls_extended_df['pos'] = topic_order_index(topic_list)\n",
    "\n",
    "lvls_extended_df['x2D'] = [v[0] for v in vec_2d]\n",
    "lvls_extended_df['y2D'] = [v[1] for v in vec_2d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lvls_extended_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "center_no = 1446\n",
    "for n in range(center_no-5, center_no+5):\n",
    "    print(n, sents[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lvls_extended_df.topic.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "topics, top_columns = display_topics(lvls_extended_df, n_rows=100, n_cols=451)\n",
    "topics = topics.fillna('')\n",
    "print('{} topics'.format(lvls_extended_df['topic'].max()))\n",
    "display(HTML(topics.to_html(index=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_fn = '../models/hobbit/agglomerative_rare3_spl_window5_list_hash0_dim100_sqrt_cca_pseudo0_ce0P75_se0'\n",
    "brown_model_df = pd.read_csv(cluster_fn, delimiter=' ', names=['cluster', 'word', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# brown_model_df[brown_model_df.term.isin(significant_terms)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "significant_terms_brown = pd.merge(significant_terms_enriched, brown_model_df, on='word', how='inner')\n",
    "significant_terms_brown = significant_terms_brown.rename(index=str, columns={\"topic\": \"cluster\", \"cluster\": \"topic\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(list(map(lambda x: len(str(x)), significant_terms_brown.topic)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def short_str(s):\n",
    "    s = str(s)\n",
    "    return s[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "significant_terms_brown['topic_s'] = list(map(short_str, significant_terms_brown['topic']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "significant_terms_brown.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_list = list(significant_terms_brown['topic'])\n",
    "significant_terms_brown['pos'] = topic_order_index(topic_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "brown_clusters = significant_terms_brown.pivot(index='pos', columns='topic', values='word').fillna('')\n",
    "display(HTML(brown_clusters.to_html(index=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brown_clusters_list = []\n",
    "for g in significant_terms_brown.groupby('topic'):\n",
    "    brown_clusters_list.append(set(g[1]['word']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_list = []\n",
    "for g in significant_terms_enriched.groupby('topic'):\n",
    "    clusters_list.append(set(g[1]['word']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(x,y):\n",
    "  \n",
    " intersection_cardinality = len(set.intersection(*[set(x), set(y)]))\n",
    " union_cardinality = len(set.union(*[set(x), set(y)]))\n",
    " return intersection_cardinality/float(union_cardinality)\n",
    "\n",
    "print(jaccard_similarity([0,1,2,5,6],[0,2,3,5,7,9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_list[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for word_cluster in clusters_list:\n",
    "    for brown_cluster in brown_clusters_list:\n",
    "        js = jaccard_similarity(word_cluster, brown_cluster)\n",
    "        if js > 0.2:\n",
    "            print(js)\n",
    "            print(word_cluster)\n",
    "            print(brown_cluster)\n",
    "            print('='*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Distributions in Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hobbit_flat_clean_bigrams.txt') as fp:\n",
    "    sents = fp.readlines()\n",
    "    sents = [s.strip() for s in sents]\n",
    "sent_ids = range(len(sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "significant_terms_enriched['weight'] = significant_terms_enriched['sigma_nor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = significant_terms_enriched.topic.max()\n",
    "word_to_topic = dict(zip(significant_terms_enriched.word, significant_terms_enriched.topic))\n",
    "word_to_weight = dict(zip(significant_terms_enriched.word, significant_terms_enriched.weight))\n",
    "    \n",
    "def message_topics(sentence):\n",
    "    ''' Calculuate the distribution of term weights in each sentence.\n",
    "        Expects a data frame that at least includes columns for word,\n",
    "        weight, and topic number. Expects lists of sentences and their\n",
    "        corresponding ids. \n",
    "    '''\n",
    "\n",
    "    sent_filtered = list(filter(lambda x: x in set(significant_terms) and word_to_topic[x] > -1, sentence.split()))\n",
    "    \n",
    "    c = Counter({n:0 for n in range(0,K)})\n",
    "    # c.update([word_to_topic[word] for word in set(sentence.split()).intersection(set(significant_terms))])\n",
    "    c.update([word_to_topic[word] for word in sent_filtered])\n",
    "    return np.array(list(c.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn off the max column width so the HTML \n",
    "# image tags don't get truncated \n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "# Turning off the max column will display all the data in\n",
    "# our arrays so limit the number of element to display\n",
    "pd.set_option('display.max_seq_items', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparkline(data, figsize=(5, 0.25), **kwags):\n",
    "    \"\"\"\n",
    "    Returns a HTML image tag containing a base64 encoded sparkline style plot\n",
    "    \"\"\"\n",
    "    legend = ' '.join([str(n) for n in data.nonzero()[0]])    \n",
    "    fig, ax = plot.subplots(1, 1, figsize=figsize, **kwags)\n",
    "#     ax.bar(range(len(data)), data)\n",
    "    ax.plot(data)\n",
    "    for k,v in ax.spines.items():\n",
    "        v.set_visible(False)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])   \n",
    "    ax.text(len(data), 0.1, legend)\n",
    "    ax.set_xlim(0,2*len(data))\n",
    "\n",
    "    ax.fill_between(range(len(data)), data, len(data)*[min(data)], alpha=0.1)\n",
    "    \n",
    "    img = io.BytesIO()\n",
    "    plot.savefig(img)\n",
    "    img.seek(0)\n",
    "    plot.close()\n",
    "    \n",
    "    short_summary = summary[summary.index.isin(legend.split())]['terms'].to_dict()\n",
    "    short_summary = '\\n'.join(c + ':' + s for c,s in [(str(key), short_summary[key]) for key in short_summary]).replace(' ','')\n",
    "                                               \n",
    "    return short_summary, '<img src=\"data:image/png;base64,{}\"/>'.format(base64.b64encode(img.read()).decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.style.use('classic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_topic = dict(zip(significant_terms_enriched['word'], significant_terms_enriched['topic']))\n",
    "word_to_weight = dict(zip(significant_terms_enriched['word'], significant_terms_enriched['weight']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = sents[278]\n",
    "c = message_topics(sentence=sent)\n",
    "print(sent)\n",
    "filtered_sent = list(filter(lambda x: x in set(significant_terms), sent.split()))\n",
    "print(filtered_sent)\n",
    "print([word_to_topic[word] for word in filtered_sent])\n",
    "print(sparkline(c)[0])\n",
    "HTML(sparkline(c)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "columns = set([word_to_topic[word] for word in filtered_sent])\n",
    "columns.remove(-1)\n",
    "topics[columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    import sentences\n",
    "    from sentences.types import Sentences\n",
    "    import numpy as np\n",
    "    \n",
    "    with open('hobbit_flat_clean_bigrams.txt') as fp:\n",
    "        text = fp.read()\n",
    "        \n",
    "    english_sentences = Sentences(text)\n",
    "    en_vectors = english_sentences.embed\n",
    "    \n",
    "    np.save('hobbit_clean_bigrams.npy', en_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_sent_vectors = np.load('hobbit_clean_bigrams.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_sent_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hobbit_flat_clean_bigrams.txt') as fp:\n",
    "    hobbit_sentences = fp.readlines()\n",
    "    \n",
    "with open('hobbit_flat.txt') as fp:\n",
    "    hobbit_sentences_orig = fp.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(hobbit_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = NearestNeighbors(metric='cosine', algorithm='brute', n_neighbors=15)\n",
    "nbrs = estimator.fit(en_sent_vectors)\n",
    "distances, indices = nbrs.kneighbors(en_sent_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hobbit_sentences_df = pd.DataFrame(hobbit_sentences, columns=['en'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_no = 9\n",
    "significant_terms_enriched[significant_terms_enriched['topic'] == cluster_no]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = hobbit_sentences_df[hobbit_sentences_df.en.str.contains('mirkwood')]\n",
    "print(len(search_results), set(search_results.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_clusters = {}\n",
    "for cluster_no in set(significant_terms_enriched['topic']):\n",
    "    terms = set(significant_terms_enriched[significant_terms_enriched['topic'] == cluster_no]['word'])\n",
    "    word_clusters[cluster_no] = terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "index_no = 1122\n",
    "print(indices[index_no])\n",
    "print(distances[index_no])\n",
    "seed_sent = list(filter(lambda x: x in set(significant_terms), hobbit_sentences[index_no].split()))\n",
    "seed_sent_clusters = set([word_to_topic[word] for word in seed_sent])\n",
    "matches = set(search_results.index).intersection(set(indices[index_no]))\n",
    "print(\"Matches: {} {}\".format(len(matches), matches) )\n",
    "print('='*50)\n",
    "for n, sent_num in enumerate(indices[index_no]):\n",
    "    if distances[index_no][n] < 0.5:\n",
    "        print(sent_num, hobbit_sentences_orig[sent_num])\n",
    "        print(sent_num, hobbit_sentences[sent_num])\n",
    "        filtered_sent = list(filter(lambda x: x in set(significant_terms), hobbit_sentences[sent_num].split()))\n",
    "        c = message_topics(hobbit_sentences[sent_num])        \n",
    "        if True:\n",
    "            print(filtered_sent)\n",
    "            print([word_to_topic[word] for word in filtered_sent])\n",
    "            filtered_sent_clusters = set([word_to_topic[word] for word in filtered_sent])\n",
    "            print(\"Jaccard similarity (seed vs. current): {}, {}\".format(jaccard_similarity(seed_sent_clusters, filtered_sent_clusters), \n",
    "                                                                         seed_sent_clusters.intersection(filtered_sent_clusters)))\n",
    "        \n",
    "        print(sparkline(c)[0])\n",
    "        display(HTML(sparkline(c)[1]))\n",
    "        print('='*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverted Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lucene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lucene\n",
    "from org.apache.lucene import analysis, document, index, queryparser, search, store, misc\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lucene.initVM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from org.apache.lucene import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = analysis.standard.StandardAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the index in memory:\n",
    "# directory = store.RAMDirectory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from java.nio.file import Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -rf ../testindex/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storeDir = \"../testindex\"\n",
    "if not os.path.exists(storeDir):\n",
    "    os.mkdir(storeDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = store.SimpleFSDirectory(Paths.get(storeDir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = document.FieldType()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexDocs(sentences, writer):\n",
    "    t1 = document.FieldType()\n",
    "    t1.setStored(True)\n",
    "    t1.setTokenized(False)\n",
    "    t1.setIndexOptions(index.IndexOptions.DOCS_AND_FREQS)\n",
    "\n",
    "    t2 = document.FieldType()\n",
    "    t2.setStored(True)\n",
    "    t2.setTokenized(True)\n",
    "    t2.setIndexOptions(index.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS)\n",
    "#     t2.setStoreTermVectorPositions()\n",
    "\n",
    "    for sent_id, sent in enumerate(sentences):\n",
    "        try:\n",
    "            doc = document.Document()\n",
    "            doc.add(document.Field(\"sent_id\", str(sent_id), t1))\n",
    "            if len(sent) > 0:\n",
    "                doc.add(document.Field(\"contents\", sent, t2))\n",
    "            writer.addDocument(doc)\n",
    "        except Exception as ex:\n",
    "            print('Exception: {}'.format(ex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hobbit_flat.txt') as fp:\n",
    "    sents = fp.readlines()\n",
    "    sents = [s.strip() for s in sents]\n",
    "sent_ids = range(len(sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = index.IndexWriterConfig(analyzer)\n",
    "iwriter = index.IndexWriter(directory, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.setSimilarity(search.similarities.ClassicSimilarity())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.setUseCompoundFile(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "iwriter.getConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "indexDocs(sents, iwriter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sent_id, sent in enumerate(sents):\n",
    "#     doc = document.Document()\n",
    "#     doc.add(document.Field('contents', sent, document.TextField.TYPE_STORED))\n",
    "#     doc.add(document.Field('sent_id', str(sent_id), document.TextField.TYPE_STORED))\n",
    "#     iwriter.addDocument(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iwriter.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now search the index:\n",
    "ireader = index.DirectoryReader.open(directory)\n",
    "isearcher = search.IndexSearcher(ireader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isearcher.setSimilarity(search.similarities.ClassicSimilarity())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse a simple query that searches for \"text\":\n",
    "parser = queryparser.classic.QueryParser('contents', analyzer)\n",
    "query = parser.parse('contents:fierce')\n",
    "hits = isearcher.search(query, 1000).scoreDocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for hit in hits:\n",
    "    hitDoc = isearcher.doc(hit.doc)\n",
    "    print(hit.score, hitDoc)\n",
    "    print(isearcher.explain(query, hit.doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ireader.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lupyne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lucene\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lucene.initVM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lupyne import engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -rf ../testindex/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storeDir = \"../testindex\"\n",
    "if not os.path.exists(storeDir):\n",
    "    os.mkdir(storeDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = engine.Indexer(directory=storeDir) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer.set('contents', engine.Field.Text, stored=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hobbit_flat.txt') as fp:\n",
    "    sents = fp.readlines()\n",
    "    sents = [s.strip() for s in sents]\n",
    "sent_ids = range(len(sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in sents:\n",
    "    indexer.add(contents=sent) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer.commit() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits = indexer.search('dragon', field='contents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for hit in hits:\n",
    "    print(hit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Topic Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [chapter.split() for chapter in chapters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [word for chapter in texts for word in chapter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(texts)\n",
    "dictionary.save(os.path.join('../models/hobbit/', 'hobbit.dict'))  # store the dictionary, for future reference\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "corpora.MmCorpus.serialize(os.path.join('../models/hobbit', 'hobbit.mm'), corpus) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary.load('../models/hobbit/hobbit.dict')\n",
    "corpus = corpora.MmCorpus('../models/hobbit/hobbit.mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.LdaModel(corpus, id2word=dictionary, num_topics=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep = pyLDAvis.gensim.prepare(model, corpus, dictionary, mds='tsne')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.display(prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
