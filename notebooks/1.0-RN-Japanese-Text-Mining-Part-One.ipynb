{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Introduction to Japanese Text Mining: Part One\n",
    "\n",
    "![Japanese Text Mining](images/japanese_text_mining.jpg)\n",
    "Check out the [Emory University workshop blog](https://scholarblogs.emory.edu/japanese-text-mining/) on Japanese Text Mining. The example notebook cells below repeat the steps in the [tutorial](http://history.emory.edu/RAVINA/JF_text_mining/Guides/Jtextmining_intro_part1.html) of Mark Ravina using python instead of R. The quoted text below is directly from Ravina's article, with minor word changes for python syntax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly_express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Structures\n",
    "\n",
    "Pandas `DataFrame` is the main python analogue of `R`'s `dataframe`.\n",
    "\n",
    "> Let’s start with a tidy, pre-processed text, the famous nineteenth-century journal Meiroku zasshi 明六雑誌。We’ll come back to the demanding task of inputting, cleaning, and “chunking” texts, but for now, let’s build on the wonderful work of NINJAL (the National Institute for Japanese Language and Linguistics), who compiled and proofed this corpus. To load the Meiroku zasshi (technically to read it into your local environment) run the code below. \n",
    "\n",
    "To run a cell of code, just get the cursor in that cell and hit the run icon in the top pane of Jupyter, or use the shortcut keys: Shift+Enter.\n",
    "\n",
    "Pandas has a number of ways to get data (locally or externally) into a data frame. Reading csv (comma-separated-values) formated files is very common. The cell below reads a csv file that is stored on a web-site hosted by the original author (Mark Ravina) at Emory University. Note that the field separator is a white-space instead of a comma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meiroku_zasshi_url = 'http://history.emory.edu/RAVINA/JF_text_mining/Guides/data/meiroku_zasshi.txt'\n",
    "Meiroku_df = pd.read_csv(meiroku_zasshi_url, sep=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command pulls the whole Meiroku zasshi into the Jupyter notebook. \n",
    "\n",
    "> As you can see, we now have the Meiroku zasshi in a data frame called Meiroku.df. A data frame is similar to a spreadsheet, although a software engineer might cringe at that statement. In a data frame, each row is a case or observation and each column is a variable. We’ll come back to those terms many times. For now, just understand that the Meiroku zasshi articles have been put into a grid, with each article and related metadata in its own row. Metadata is information about the text, such as the author.\n",
    "\n",
    "> We identify columns of a data frame by combining the name of the data frame and the name of the column, joined by the `.` mark. The `.` mark tells Python that a given column (or vector) is associated with a specific dataframe. `Meiroku_df.author` means the column `author` in the dataframe `Meiroku_df`.\n",
    "\n",
    "Common ways to inspect a dataframe are to look at the `head` or `tail` of the underlying table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Meiroku_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, when you select a column, pandas returns a `Series` object instead of a `DataFrame`, so beware. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Meiroku_df.author.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> You’ll note that the output is the list of Meiroku zasshi authors, with multiple appearances if they wrote more than one article. The output is 155 elements long because there are 155 articles.\n",
    "\n",
    "Extract a (series) of author names from the dataframe, and then make a unique list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Meiroku_df.author.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The vector `Meiroku_df.author` is a one dimensional data object, so if we want to grab a single element we just need one number. We indicate the element’s location using square brackets. Thus, for the author of the second Meiroku zasshi article:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Meiroku_df.author[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select rows 2 thru 5 (inclusive): `Meiroku_df.author[1:5]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Meiroku_df.author.iloc[1:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Data frames are two-dimensional objects, so identifying an element requires two markers, first the row number(s), then the column name(s). The author information is in the 4th column of the data frame Meiroku_df, so to get the author of the second article:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Meiroku_df.loc[2, 'author']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Odd shaped selections of rows can be build up as a union (`|`) of sets and used as part of the dataframe selection argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = {2} | set(range(6,9)) | {10,12}\n",
    "print(rows)\n",
    "Meiroku_df.loc[rows, ('title', 'author')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Meiroku_df.loc[:, 'author']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Meiroku_df.loc[1:6, 'year']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment and Subsetting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Let’s build on that basic knowledge of vectors, by asking Python about `Meiroku_df.author`. For example, which elements of `Meiroku_df` author are equal to Nishi Amane 西周? Note that the interrogative requires a double equals sign. (As an aside, a single equals sign is more of a command, telling Python to make `Meiroku_df.author` equal to Nishi Amane. That would actually overwrite all the author values! So . . ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author Nishi Amane\n",
    "special_subset = Meiroku_df.author == '西周'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Python answers our query with a logical vector: a series of TRUE/FALSE responses. Nishi Amane is the author of the first element, but not the second, etc. This is accurate, but not especially useful. We can, however, use this information to get the subset of the dataframe elements for which the answer is TRUE.\n",
    "\n",
    "> Now we can use that vector to get the titles of all the Meiroku zasshi articles written by Nishi Amane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Meiroku_df[special_subset].title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Meiroku_df.year[special_subset]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Remember that the syntax for a data frame is\n",
    "\n",
    "> `Name_of_data_frame[row_number, column_name]`\n",
    "\n",
    "> and that a nothing after the comma means “everything.” So we just took all of the columns of Meiroku.df but just some of the rows. If you want a denser syntax, you can skip the intermediate step of creating the vector special_subset. Just put the selection criteria right in the brackets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nishi_articles_df = Meiroku_df[Meiroku_df.author == '西周']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Programmers love dense code like that and they esteem “one-liners,” extremely compact, powerful code snippets. But, at least at first, it can be much easier to code in small incremental steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions and more Subsetting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In order to do more sophisticated text mining, we’ll rely on some packages and their functions. Setting aside the technicalities, functions are commands and packages are bundles of related functions. In order to use a package we need to install it once, but load it each time we restart Python or otherwise clear the Python environment. By way of extended metaphor, installed packages is like having the library buy a book. By contrast library gets the book and opens it on your desk. For the string package,the commands are:\n",
    "\n",
    "Available methods for the python `string` class ignoring `dunder` (double underscore) functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function = str\n",
    "print([s for s in dir(function) if '__' not in s])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The string class has a series of logically named methods for handling strings, a technical term for alphanumeric text. A good example of such a simple, logical function is `str.count`. What do you suppose this command does?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = Meiroku_df.text.str.count('女') != 0\n",
    "Meiroku_df['女'] = Meiroku_df.text.str.count('女')\n",
    "Meiroku_df[mask].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note that this command counts the character 女 both alone and in longer compounds such as 男女 and 女性. We’ll explore methods for refining that search soon. For now, as an interim method, you can add whitespace and search for \" 女 “. That will miss the occassional cases of 女 at the beginning or end of a sentence, or (if there’s punctuation) before a period or comma. So we’ll cover a more refined method of search in the next session.\n",
    "\n",
    "> Rather than just let the results of str_count hang loose, we can add them to the data frame Meiroku_df, creating a new column called 女. Use the `.` operator to put the vector in the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = Meiroku_df.text.str.count('女') != 0\n",
    "Meiroku_df['女'] = Meiroku_df.text.str.count(' 女 ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Meiroku_df[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We can now use the same tricks as before to subset a data frame. Let’s select every essay in the Meiroku zasshi that used the characters 女 more than 自由"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = Meiroku_df.text.str.count('自由') != 0\n",
    "Meiroku_df['自由'] = Meiroku_df.text.str.count('自由')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are {} articles containing the string \" 女 \" and {} articles containing \"自由\".'.format(\n",
    "    len(Meiroku_df[Meiroku_df['女'] > 0]), len(Meiroku_df[Meiroku_df['自由'] >  0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Meiroku_subset_df = Meiroku_df[Meiroku_df.女 > Meiroku_df.自由]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Meiroku_subset_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can, of course, add additional criteria, such as choosing only works by Mori Arinori that use 女 more than 自由. We can either subset in several steps . . ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (Meiroku_df.text.str.count('女') > Meiroku_df.text.str.count('自由'))\n",
    "mask = mask & (Meiroku_df.author == '森有礼')\n",
    "Meiroku_df[mask].title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> You can also combine conditions with the “or” operator `|` , the uppercase version of the “backslash.” If you want the titles of essays written by either Mori Arinori or Katō Hiroyuki."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (Meiroku_df.author == '森有礼') | (Meiroku_df.author == '加藤弘之')\n",
    "Meiroku_df[mask].title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Take a moment to experiment with subsetting, creating new variables, and specifying multiple criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colocation — A basic data visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We’re now going to shift from straightforward, simple code to some dense, advanced commands. That means, for now, just focusing on a few key arguments and ignoring other parts of the command. Python has some wonderful packages for visualizing data. We’ll use plotly express, a great interactive graphing package. As with all packages, you’ll need to install them once, but only once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = Meiroku_df.text.str.count(' 女 ') != 0\n",
    "Meiroku_df['女'] = Meiroku_df.text.str.count('女')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(Meiroku_df, x='女', y='自由', hover_name='author')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note that if you want to visualize a term, you need to first get the word count. If, for example, we want to plot 女 against 男, we need the count for 男."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = Meiroku_df.text.str.count('男') != 0\n",
    "Meiroku_df['男'] = Meiroku_df.text.str.count('男')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now let’s reuse the plotting code, but replacing 自由 with 男"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(Meiroku_df, x='女', y='男', hover_name='author')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> As you may have noticed, these graphs do not contain 155 points because some of the articles have the exact same values. This problem is called overplotting: we can’t see some of the observations because they are underneath other observations with the same value.\n",
    "\n",
    "> In this case, we can fix the problem by counting the words as percentages of the total characters in each article. That’s sometimes called “normalizing.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nchar(text):\n",
    "    '''Counts the number of characters in the string (less spaces).'''\n",
    "    text = text.replace(' ', '')\n",
    "    return len(list(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Meiroku_df['女'] = Meiroku_df.text.apply(lambda x: x.count('女')/nchar(x)*100)\n",
    "Meiroku_df['男'] = Meiroku_df.text.apply(lambda x: x.count('男')/nchar(x)*100)a\n",
    "Meiroku_df['自由'] = Meiroku_df.text.apply(lambda x: x.count('自由')/nchar(x)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(Meiroku_df, x='女', y='男', hover_name='author')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note that any numeric variable can be used for x and y in `plotly`, so here’s how the usage of 女 varied over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(Meiroku_df, x='issue', y='女', hover_name='author')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DTM: Document-term matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We now have some fairly powerful tools, but these methods are somewhat labor-intensive. There are over 15,000 unique words in the Meiroku zasshi and it would be cumbersome to write 15,000 lines of code by hand, one for each term.\n",
    "\n",
    "> Fortunately, Python loves to help with repetitive tasks so we can write 7 or 8 lines of code instead of 15,000. Unfortunately, some of that code is rather advanced, so, for now at least, you’ll just have to use the commands without fully understanding the details. Much of the complexity below involves turning lists and matrices into data frames. We’ll get to those more conceptual issues later.\n",
    "\n",
    "> For now we’ll need a list of all the unique terms in the Meiroku zasshi. To get that, we’ll need to smash all the individual articles together into one long string. We’ll use the command `join`, the text equivalent of addition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Because the individual articles are elements of a vector, we need to use the join function. Note how this “joins” all the articles into one long string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_meiroku = ' '.join(Meiroku_df.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now we can split the string into individual words, separating on whitespace. The command str.split is appropriately named: it splits strings. The first line below should therefore be obvious. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_meiroku_split = complete_meiroku.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(complete_meiroku_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The object all_words is now a vector with ~~173,197~~ 172,875 elements, the total word count for all 155 articles. To get a list of unique words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meiroku_unique_words = set(complete_meiroku_split)\n",
    "len(meiroku_unique_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note that `meiroku_unique_words` is much smaller: only ~~15,603~~ 15,601 elements. Another handy class is `Counter`, which quickly and easily calculates the frequency of every word in the Meiroku zasshi. Note that you can easily sort this data frame by frequency.\n",
    "\n",
    "Note: Attempt to resolve different values for counts. Notice unicode value \\u3000 showing up in the text so tried normalizing. May also need to account for all white space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    return unicodedata.normalize('NFKC', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Meiroku_df['text'] = Meiroku_df.text.map(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = ' '.join(Meiroku_df.text)\n",
    "all_words = all_text.split()\n",
    "print(len(all_words), len(set(all_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = Counter(all_words)\n",
    "Meiroku_frequency_df = pd.DataFrame.from_dict(counts, orient='index').reset_index()\n",
    "Meiroku_frequency_df.columns = ['word', 'count']\n",
    "Meiroku_frequency_df = Meiroku_frequency_df.sort_values(by='count', ascending=False)\n",
    "Meiroku_frequency_df['term index'] = list(range(1,len(Meiroku_frequency_df)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(Meiroku_frequency_df, x='term index', y='count', \n",
    "                 hover_name='word', log_x=True, log_y=True)\n",
    "fig.layout.title = 'Total Vocabulary {}'.format(len(set(all_words)))\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_length(text):\n",
    "    return len(text.split())\n",
    "\n",
    "Meiroku_df['text_length'] = Meiroku_df.text.map(text_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_frequency(text):\n",
    "    counts = Counter({word:0 for word in Meiroku_frequency_df.word})\n",
    "    counts.update(text.split())\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Meiroku_df['word_counts'] = Meiroku_df.text.map(text_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm = pd.DataFrame.from_dict(list(Meiroku_df.word_counts.values))\n",
    "dtm = dtm[Meiroku_frequency_df.word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (Meiroku_frequency_df['count'] == 10)\n",
    "Meiroku_frequency_df[mask].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm[dtm.朝鮮 > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# document_index = 46\n",
    "# Meiroku_df.text.loc[document_index + 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregating and Sorting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s do some final manipulation of the document term matrix, aggregating by author. Which authors favored which words? First, let’s see how many authors wrote for the Meiroku zasshi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(Meiroku_df.author)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s aggregate the word frequencies by author. We’ll get the total word count for each author, and then “renormalize” the dtm. We’ll create a dataframe temp.df that just has the author names and the word counts. One catch is that the names of the authors are non-numeric, so we’ll need to tell R not to do math on the author names! First we’ll create a new data frame with the author’s names and the word counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Meiroku_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm['author'] = list(Meiroku_df.author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dtm.groupby('author').sum(axis=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normalized = df.apply(lambda x: x/x.sum(), axis=1)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normalized.自由.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
