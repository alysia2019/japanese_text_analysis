{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Introduction to Japanese Text Mining: Part Three\n",
    "\n",
    "![Japanese Text Mining](images/japanese_text_mining.jpg)\n",
    "Check out the [Emory University workshop blog](https://scholarblogs.emory.edu/japanese-text-mining/) on Japanese Text Mining. The example notebook cells below repeat the steps in the [tutorial](http://history.emory.edu/RAVINA/JF_text_mining/Guides/Jtextmining_intro_part2.html) of Mark Ravina using python instead of R. The quoted text below is directly from Ravina's article, with minor word changes for python syntax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly_express as px\n",
    "import unicodedata\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Basic Statistical Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In the following session we will learn several techniques for comparing large numbers of documents. All of these techniques rely on the vector space model of documents, which sees each text as a vector of values corresponding to the words in that text. We will learn how to manipulate word vectors, how to preprocess them for analysis, and how to visualize their distances from one another using a variety of clustering algorithms. Finally, we will explore one approach for identifying words that distinguish between two groups of texts. This workbook assumes that you’ve already completed Parts 1 and 2 and builds on the programming skills you developed there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data\n",
    "\n",
    "> To begin, we will load the Meiroku Zasshi data as we did in Part 2 of this tutorial. This means grabbing the texts and their associated metadata and producing a document-term-matrix (DTM) with raw counts of every word across all 155 texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_frequency(text):\n",
    "    counts = Counter({word:0 for word in Meiroku_frequency_df.word})\n",
    "    counts.update(text.split())\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meiroku_zasshi_url = 'http://history.emory.edu/RAVINA/JF_text_mining/Guides/data/meiroku_zasshi.txt'\n",
    "Meiroku_df = pd.read_csv(meiroku_zasshi_url, sep=' ')\n",
    "complete_meiroku = ' '.join(Meiroku_df.text)\n",
    "complete_meiroku_split = complete_meiroku.split()\n",
    "meiroku_unique_words = set(complete_meiroku_split)\n",
    "all_words = complete_meiroku.split()\n",
    "\n",
    "counts = Counter(all_words)\n",
    "Meiroku_frequency_df = pd.DataFrame.from_dict(counts, orient='index').reset_index()\n",
    "Meiroku_frequency_df.columns = ['word', 'count']\n",
    "Meiroku_frequency_df = Meiroku_frequency_df.sort_values(by='count', ascending=False)\n",
    "Meiroku_frequency_df['term index'] = list(range(1,len(Meiroku_frequency_df)+1))\n",
    "\n",
    "Meiroku_df['text'] = Meiroku_df.text.map(lambda x: unicodedata.normalize('NFKC', x))\n",
    "Meiroku_df['word_counts'] = Meiroku_df.text.map(text_frequency)\n",
    "dtm_df = pd.DataFrame.from_dict(list(Meiroku_df.word_counts.values))\n",
    "dtm_df = dtm_df[Meiroku_frequency_df.word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_df.loc[0, ['洋字','を','以','て','國語','書する','の','論','西']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Each row in our document-term-matrix can be treated as a word vector. A vector is simply a sequence of numbers wherein each number represents the value of some variable (or attribute) associated with a particular data point. In this case, the variable is the raw frequency of a particular word. Seen this way, our document-term-matrix is a collection of 155 vectors and 15,603 variables (i.e., the number of unique words in the corpus). What’s great about these vector representations of each text is that we now have a way to compare many texts via mathematical operations. For each text is now an arrow pointing out into a high dimensional space (a space with 15,603 dimensions!) and there are easy ways to compare the distances between arrows based on where they point in this space. This is known as the vector space model.\n",
    "\n",
    "> Within the vector space model, there are multiple ways of manipulating the vectors in order to compare them. We will look at the following methods in this part of the tutorial:\n",
    "* Euclidean Distance\n",
    "* Cosine Similarity\n",
    "* K-Means Clustering\n",
    "\n",
    "> The first two are closely related and rely on the geometrical properties inherent to vectors. If we think of these vectors as pointing out into n-dimensional space, then Euclidean Distance and Cosine Similarity are two different ways of measuring how close any two vectors are in this space. They differ, however, in that Euclidean is more sensitive to length than Cosine because the former measures distance between the endpoints of the vectors. Thus one must be careful to normalize one’s vectors before using Euclidean distance, otherwise the differences between texts may only reflect their size (e.g., the more words in a text, the further its vector will point).\n",
    "\n",
    "> Cosine similarity, in contrast, measures the cosine of the angle between two vectors and is thus not susceptible to differences in length. It is a similarity measure, not a distance measure, and ranges from 0 to 1. The closer to 1 it is, the more similar two word vectors are. To express as a distance, we subtract the cosine similarity from 1.\n",
    "\n",
    "> This brings us to an important point about the importance of normalizing vectors. We want to be sure that we’re comparing texts on the same scale. We did this previously when we normalized our word counts by document length, producing relative frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_norm_matrix = dtm_df.apply(lambda x: x/x.sum(), axis=1)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.166985/0.03972082"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_norm_matrix.loc[0,['洋字','を','以','て','國語','書する','の','論','西']]/4.2039665847784615"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_norm_matrix = dtm_norm_matrix/4.2039665847784615"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> But there are other ways to normalize our texts depending on the kind of information we want to extract from them. One problem with relative frequencies is that they give a lot of weight to high frequency terms (e.g., like the particle を). A lot of high frequency words, however, are not distinguishing features between texts. If lots of documents use the particle を, then it doesn’t tell us a great deal about differences between documents. One method for down weighting such terms is the term-frequency inverse-document frequency (tf-idf) method. This method re-weights words according to how often they occur across a corpus, giving less weight to terms that appear in many documents and more weight to terms that appear in only a few documents. Here’s the formula:\n",
    "\n",
    "> The logarithm in the denominator ensures that rarer terms will be given a higher weight. Here’s code to re-produce our DTM with the tf-idf weights instead of the relative frequencies. We should see that the values for some of the common, high frequency words is now lower than the values for rarer, less frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = dtm_df.copy()\n",
    "dtm_norm_matrix = tf.apply(lambda x: x/x.sum(), axis=1)\n",
    "idf = np.log10(len(tf)/tf.astype(bool).sum(axis=0))\n",
    "tfidf = dtm_norm_matrix*idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf.loc[0,['洋字','を','以','て','國語','書する','の','論','西']]/4.20396"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toy TFIDF Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docA = \"The cat sat on my face\"\n",
    "docB = \"The dog sat on my bed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: include doc_frequency_df as a parameter.\n",
    "def text_frequency2(text):\n",
    "    counts = Counter({word:0 for word in doc_frequency_df.word})\n",
    "    counts.update(text.split())\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_doc = ' '.join([docA, docB])\n",
    "complete_doc_split = complete_doc.split()\n",
    "doc_unique_words = set(complete_doc_split)\n",
    "all_words = complete_doc.split()\n",
    "\n",
    "counts = Counter(all_words)\n",
    "doc_frequency_df = pd.DataFrame.from_dict(counts, orient='index').reset_index()\n",
    "doc_frequency_df.columns = ['word', 'count']\n",
    "doc_frequency_df = doc_frequency_df.sort_values(by='count', ascending=False)\n",
    "doc_frequency_df['term index'] = list(range(1,len(doc_frequency_df)+1))\n",
    "\n",
    "doc_df = pd.DataFrame([docA, docB], columns=['text'])\n",
    "doc_df['word_counts'] = doc_df.text.map(text_frequency2)\n",
    "dtm_df = pd.DataFrame.from_dict(list(doc_df.word_counts.values))\n",
    "# dtm_df = dtm[Meiroku_frequency_df.word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_norm_matrix = dtm_df.apply(lambda x: x/x.sum(), axis=1)\n",
    "dtm_norm_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_norm_matrix.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The document frequency is how many documents each term shows up in. It should be a number between 1 and the number of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_df.astype(bool).sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inverse document frequency, `idf` normalizes the count by dividing by the total number of documents, inverts that ratio, and then take the logarithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = np.log10(len(dtm_df)/dtm_df.astype(bool).sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_norm_matrix*idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
