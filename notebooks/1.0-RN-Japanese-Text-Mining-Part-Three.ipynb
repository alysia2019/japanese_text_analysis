{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Introduction to Japanese Text Mining: Part Three\n",
    "\n",
    "![Japanese Text Mining](images/japanese_text_mining.jpg)\n",
    "Check out the [Emory University workshop blog](https://scholarblogs.emory.edu/japanese-text-mining/) on Japanese Text Mining. The example notebook cells below repeat the steps in the [tutorial](http://history.emory.edu/RAVINA/JF_text_mining/Guides/Jtextmining_intro_part2.html) of Mark Ravina using python instead of R. The quoted text below is directly from Ravina's article, with minor word changes for python syntax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly_express as px\n",
    "import unicodedata\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Basic Statistical Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In the following session we will learn several techniques for comparing large numbers of documents. All of these techniques rely on the vector space model of documents, which sees each text as a vector of values corresponding to the words in that text. We will learn how to manipulate word vectors, how to preprocess them for analysis, and how to visualize their distances from one another using a variety of clustering algorithms. Finally, we will explore one approach for identifying words that distinguish between two groups of texts. This workbook assumes that you’ve already completed Parts 1 and 2 and builds on the programming skills you developed there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data\n",
    "\n",
    "> To begin, we will load the Meiroku Zasshi data as we did in Part 2 of this tutorial. This means grabbing the texts and their associated metadata and producing a document-term-matrix (DTM) with raw counts of every word across all 155 texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_frequency(text):\n",
    "    counts = Counter({word:0 for word in Meiroku_frequency_df.word})\n",
    "    counts.update(text.split())\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bad_text_frequency(text):\n",
    "    '''\n",
    "    Illustrate error in counting charcter matches. Use this \n",
    "    frequency count to reproduce exact results of tutorial.\n",
    "    \n",
    "    Below essentially reproduces the computation in the str_count R code (counting regex matches):\n",
    "      dtm.matrix <- sapply(X = Meiroku.unique.words, \n",
    "                           FUN = function(x) str_count(string = Meiroku.df$text, pattern = x)\n",
    "                        )\n",
    "      dtm.df <- as.data.frame(dtm.matrix)\n",
    "    '''\n",
    "    counts = {word:0 for word in Meiroku_frequency_df.word}\n",
    "    for word in counts:\n",
    "        matches = re.findall('{}'.format(word), text)\n",
    "        counts[word] = len(matches)\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meiroku_zasshi_url = 'http://history.emory.edu/RAVINA/JF_text_mining/Guides/data/meiroku_zasshi.txt'\n",
    "Meiroku_df = pd.read_csv(meiroku_zasshi_url, sep=' ')\n",
    "complete_meiroku = ' '.join(Meiroku_df.text)\n",
    "complete_meiroku_split = complete_meiroku.split()\n",
    "meiroku_unique_words = set(complete_meiroku_split)\n",
    "all_words = complete_meiroku.split()\n",
    "\n",
    "counts = Counter(all_words)\n",
    "Meiroku_frequency_df = pd.DataFrame.from_dict(counts, orient='index').reset_index()\n",
    "Meiroku_frequency_df.columns = ['word', 'count']\n",
    "Meiroku_frequency_df = Meiroku_frequency_df.sort_values(by='count', ascending=False)\n",
    "Meiroku_frequency_df['term index'] = list(range(1,len(Meiroku_frequency_df)+1))\n",
    "\n",
    "# TODO: Track down if R automatically normalized text (esp. \\u3000 codes).\n",
    "Meiroku_df['text'] = Meiroku_df.text.map(lambda x: unicodedata.normalize('NFKC', x))\n",
    "\n",
    "# Note: Use the bad_text_frequency to reproduce tutorial results.\n",
    "Meiroku_df['word_counts'] = Meiroku_df.text.map(text_frequency)\n",
    "dtm_df = pd.DataFrame.from_dict(list(Meiroku_df.word_counts.values))\n",
    "dtm_df = dtm_df[Meiroku_frequency_df.word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrepancy in Numeric values of Python and R Notebooks.\n",
    "\n",
    "Results from the tutorial for the document term counts:\n",
    "```\n",
    "##   洋字  を 以  て 國語 書する  の 論 　 西\n",
    "## 1    7 282 29 188    6      2 276  9 22  1\n",
    "```\n",
    "\n",
    "Python results on counting already tokenized text by splitting on whitespace. Can reproduce the results in the tutorial by using the `bad_text_frequency` routine. \n",
    "\n",
    "Basic issue is counting regex matches instead of tokenized characters, so substrings get overcounted.\n",
    "```\n",
    "洋字       7\n",
    "を      281\n",
    "以       24\n",
    "て      129\n",
    "國語       6\n",
    "書する      2\n",
    "の      249\n",
    "論        4\n",
    "西        1\n",
    "Name: 0, dtype: int64\n",
    "```\n",
    "\n",
    "Appears that `R` code was finding substring matches instead of whole word matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = Meiroku_df.text.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in ['洋字','を','以','て','國語','書する','の','論','西']:\n",
    "    matches = re.findall('.{}.'.format(word), text)\n",
    "    # print(len(matches), matches)\n",
    "\n",
    "    discrepancy = len([m.strip() for m in matches if (len(m.strip()) - len(word)) > 0])\n",
    "    print(word, discrepancy, ' + ', dtm_df.loc[0, word], ' = ', discrepancy + dtm_df.loc[0, word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_split = []\n",
    "word = 'て'\n",
    "for w in text.split():\n",
    "    if w == word:\n",
    "        matches_split.append(w)\n",
    "\n",
    "len(matches_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_split = []\n",
    "for w in re.split(r'\\W+', text):\n",
    "    if w == word:\n",
    "        matches_split.append(w)\n",
    "        \n",
    "len(matches_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_df.loc[0, ['洋字','を','以','て','國語','書する','の','論','西']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Each row in our document-term-matrix can be treated as a word vector. A vector is simply a sequence of numbers wherein each number represents the value of some variable (or attribute) associated with a particular data point. In this case, the variable is the raw frequency of a particular word. Seen this way, our document-term-matrix is a collection of 155 vectors and 15,603 variables (i.e., the number of unique words in the corpus). What’s great about these vector representations of each text is that we now have a way to compare many texts via mathematical operations. For each text is now an arrow pointing out into a high dimensional space (a space with 15,603 dimensions!) and there are easy ways to compare the distances between arrows based on where they point in this space. This is known as the vector space model.\n",
    "\n",
    "> Within the vector space model, there are multiple ways of manipulating the vectors in order to compare them. We will look at the following methods in this part of the tutorial:\n",
    "* Euclidean Distance\n",
    "* Cosine Similarity\n",
    "* K-Means Clustering\n",
    "\n",
    "> The first two are closely related and rely on the geometrical properties inherent to vectors. If we think of these vectors as pointing out into n-dimensional space, then Euclidean Distance and Cosine Similarity are two different ways of measuring how close any two vectors are in this space. They differ, however, in that Euclidean is more sensitive to length than Cosine because the former measures distance between the endpoints of the vectors. Thus one must be careful to normalize one’s vectors before using Euclidean distance, otherwise the differences between texts may only reflect their size (e.g., the more words in a text, the further its vector will point).\n",
    "\n",
    "> Cosine similarity, in contrast, measures the cosine of the angle between two vectors and is thus not susceptible to differences in length. It is a similarity measure, not a distance measure, and ranges from 0 to 1. The closer to 1 it is, the more similar two word vectors are. To express as a distance, we subtract the cosine similarity from 1.\n",
    "\n",
    "> This brings us to an important point about the importance of normalizing vectors. We want to be sure that we’re comparing texts on the same scale. We did this previously when we normalized our word counts by document length, producing relative frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_norm_df = dtm_df.apply(lambda x: x/x.sum(), axis=1)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_norm_df.loc[0, ['洋字','を','以','て','國語','書する','の','論','西']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> But there are other ways to normalize our texts depending on the kind of information we want to extract from them. One problem with relative frequencies is that they give a lot of weight to high frequency terms (e.g., like the particle を). A lot of high frequency words, however, are not distinguishing features between texts. If lots of documents use the particle を, then it doesn’t tell us a great deal about differences between documents. One method for down weighting such terms is the term-frequency inverse-document frequency (tf-idf) method. This method re-weights words according to how often they occur across a corpus, giving less weight to terms that appear in many documents and more weight to terms that appear in only a few documents. Here’s the formula:\n",
    "\n",
    "> The logarithm in the denominator ensures that rarer terms will be given a higher weight. Here’s code to re-produce our DTM with the tf-idf weights instead of the relative frequencies. We should see that the values for some of the common, high frequency words is now lower than the values for rarer, less frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = dtm_df.copy()\n",
    "dtm_norm_matrix = tf.apply(lambda x: x/x.sum(), axis=1)\n",
    "\n",
    "# Note: Tutorial R based code uses base 2 log.\n",
    "idf = np.log(len(tf)/tf.astype(bool).sum(axis=0))\n",
    "# tfidf = dtm_norm_matrix*idf\n",
    "tfidf = tf*idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf.loc[0,['洋字','を','以','て','國語','書する','の','論','西']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toy TFIDF Example\n",
    "\n",
    "From [github project](https://github.com/mayank408/TFIDF/blob/master/TFIDF.ipynb) of Mayank Tripathi. (Love google search.) Differs from the tutorial by using a base 10 log and multiplying the inverse document frequency by the __normalized__ term frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docA = \"The cat sat on my face\"\n",
    "docB = \"The dog sat on my bed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: include doc_frequency_df as a parameter.\n",
    "def toy_text_frequency(text):\n",
    "    counts = Counter({word:0 for word in toy_frequency_df.word})\n",
    "    counts.update(text.split())\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_doc = ' '.join([docA, docB])\n",
    "complete_doc_split = complete_doc.split()\n",
    "doc_unique_words = set(complete_doc_split)\n",
    "toy_all_words = complete_doc.split()\n",
    "\n",
    "toy_counts = Counter(toy_all_words)\n",
    "toy_frequency_df = pd.DataFrame.from_dict(toy_counts, orient='index').reset_index()\n",
    "toy_frequency_df.columns = ['word', 'count']\n",
    "toy_frequency_df = toy_frequency_df.sort_values(by='count', ascending=False)\n",
    "toy_frequency_df['term index'] = list(range(1,len(toy_frequency_df)+1))\n",
    "\n",
    "doc_df = pd.DataFrame([docA, docB], columns=['text'])\n",
    "doc_df['word_counts'] = doc_df.text.map(toy_text_frequency)\n",
    "toy_dtm_df = pd.DataFrame.from_dict(list(doc_df.word_counts.values))\n",
    "# dtm_df = dtm[Meiroku_frequency_df.word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_dtm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_dtm_norm_matrix = toy_dtm_df.apply(lambda x: x/x.sum(), axis=1)\n",
    "toy_dtm_norm_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the rows are normalized to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_dtm_norm_matrix.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The document frequency is how many documents each term shows up in. It should be a number between 1 and the number of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Favorite way to count nonzero elements in column (axis 0).\n",
    "toy_dtm_df.astype(bool).sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inverse document frequency, `idf` normalizes the count by dividing by the total number of documents, inverts that ratio, and then take the logarithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_idf = np.log10(len(toy_dtm_df)/toy_dtm_df.astype(bool).sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_dtm_norm_matrix*toy_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "\n",
    "> Once you’ve normalized your texts into comparable units, the next question to consider is whether you need all the dimensions that are available to you. Or to put it another way, is there a way to reduce the dimensions to filter out some of the noise introduced by having so many words? All of the words may not be important to the question you are trying to answer. This process of reducing the dimensions is called feature selection. Here are some common methods for reducing features:\n",
    "* Retain only those words with an average frequency above some value (i.e., keep most frequent terms).\n",
    "* Filter out the grammatical function words (or stopwords). These can be good indicators of authorial style, but they are less useful for identifying differences in content. Lists of stopwords should be tailored to whatever corpus you are working with and are typically created using the most frequent words in the corpus.\n",
    "* Lemmatize or stem the words in your corpus, reducing them to their base forms. In this way, one can collapse all of the variants of a word into a single term. Lemma can easily be extracted from the MeCab/Unidic output, as discussed in a previous session.\n",
    "\n",
    "> Here, we will perform two kinds of feature selection on the corpus before proceeding with our analysis. First we will filter out stopwords from the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To build a list of stopwords, let's first get the highest frequency words.\n",
    "# Build a frequency table from the Meiroku data.\n",
    "all_words = complete_meiroku.split()\n",
    "counts = Counter(all_words)\n",
    "Meiroku_frequency_df = pd.DataFrame.from_dict(counts, orient='index').reset_index()\n",
    "Meiroku_frequency_df.columns = ['word', 'count']\n",
    "Meiroku_frequency_df = Meiroku_frequency_df.sort_values(by='count', ascending=False)\n",
    "Meiroku_frequency_df['term index'] = list(range(1,len(Meiroku_frequency_df)+1))\n",
    "Meiroku_frequency_df = Meiroku_frequency_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Meiroku_frequency_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Meiroku_frequency_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = list(Meiroku_frequency_df.word.loc[0:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nostop_norm_df = dtm_norm_df.copy()\n",
    "nostop_norm_df.drop(stopwords, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nostop_tfidf = tfidf.copy()\n",
    "nostop_tfidf.drop(stopwords, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nostop_norm_df.shape, nostop_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Next, let’s reduce the features by keeping only those words that occur more than some threshold for mean relative frequency or mean tf-idf score.\n",
    "\n",
    "`#keep only those columns where the mean relative frequency of a word is >= .05`\n",
    "\n",
    "`reduced.norm.df <- nostop.norm.df[,apply(nostop.norm.df, 2, mean) >= .05]`\n",
    "\n",
    "`#check to see how many features you have; this still seems like a lot so we will reduce further`\n",
    "\n",
    "`dim(reduced.norm.df)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (nostop_norm_df.mean(axis=0) > 0.05)\n",
    "columns = nostop_norm_df.columns[mask]\n",
    "nostop_norm_df[columns].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (nostop_norm_df.mean(axis=0) > 0.01)\n",
    "columns = nostop_norm_df.columns[mask]\n",
    "nostop_norm_df[columns].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (nostop_norm_df.mean(axis=0) > 0.003)\n",
    "columns = nostop_norm_df.columns[mask]\n",
    "reduced_norm_df = nostop_norm_df[columns]\n",
    "reduced_norm_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (nostop_tfidf.mean(axis=0) > 0.3)\n",
    "columns = nostop_tfidf.columns[mask]\n",
    "reduced_tfidf_df = nostop_tfidf[columns]\n",
    "reduced_tfidf_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Distances\n",
    "\n",
    "> We are finally ready to start comparing our texts. Since 155 texts are a lot to visualize all at once, we will just look at the first 50 titles. Let’s also grab the corresponding metadata for these works since we will need them to label our visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = Meiroku_df.author\n",
    "titles = Meiroku_df.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just to make things interesting, we'll obscure one of the authors names\n",
    "authors[12] = \"Mystery Author\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now we create a distance matrix using the function dist(). This function will do a pairwise comparison of all the texts in our dataset (every text against every other) and store the results as a large matrix. This function will use Euclidean Distance to calculate distances. Let’s create a second distance matrix using Cosine Similarity. For this, we need to borrow a function from another R library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a distance matrix using the relative frequency DTM.\n",
    "dist = lambda p1, p2: sqrt(((p1-p2)**2).sum())\n",
    "dm = np.asarray([[dist(p1, p2) for p2 in xy_list] for p1 in xy_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_norm_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist\n",
    "import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_norm_matrix = reduced_norm_df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euc_dist_matrix = pdist(reduced_norm_matrix, metric='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP()\n",
    "embedding = reducer.fit_transform(reduced_norm_matrix)\n",
    "print(embedding.shape)\n",
    "\n",
    "embedding_df = pd.DataFrame(embedding, columns=['x', 'y'])\n",
    "embedding_df['authors'] = authors\n",
    "embedding_df['title'] = titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(embedding_df, x='x', y='y', hover_name='authors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_authors = list(Meiroku_df.author.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (embedding_df.authors == unique_authors[0])\n",
    "embedding_df[mask].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.FigureWidget()\n",
    "for author in unique_authors:\n",
    "    mask = (embedding_df.authors == author)\n",
    "    scatter = fig.add_scatter(x=embedding_df[mask].x, y=embedding_df[mask].y)\n",
    "    scatter.name = author\n",
    "    scatter.mode = 'markers'\n",
    "    scatter.hovertext = embedding_df[mask].title\n",
    "    scatter.hoverinfo = 'x+y+text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (embedding_df.x < 3.5) & (embedding_df.y > 1)\n",
    "embedding_df[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP()\n",
    "reduced_tfidf_matrix = reduced_tfidf_df.to_numpy()\n",
    "embedding = reducer.fit_transform(reduced_tfidf_matrix)\n",
    "print(embedding.shape)\n",
    "\n",
    "embedding_df = pd.DataFrame(embedding, columns=['x', 'y'])\n",
    "embedding_df['authors'] = authors\n",
    "embedding_df['title'] = titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_tfidf = go.FigureWidget()\n",
    "for author in unique_authors:\n",
    "    mask = (embedding_df.authors == author)\n",
    "    scatter = fig_tfidf.add_scatter(x=embedding_df[mask].x, y=embedding_df[mask].y)\n",
    "    scatter.name = author\n",
    "    scatter.mode = 'markers'\n",
    "    scatter.hovertext = embedding_df[mask].title\n",
    "    scatter.hoverinfo = 'x+y+text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (embedding_df.y < -7.3)\n",
    "embedding_df[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
